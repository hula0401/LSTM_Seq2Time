{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d9f672-e3f7-42f0-a4ad-abfb34d88731",
   "metadata": {},
   "source": [
    "# Sequential Peptide Retention Time Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c8de7-2148-476b-9481-92a64fd246a0",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "* Sequence to value -- Peptide sequence to a time point (0~180 min)\n",
    "### Background:\n",
    "* ESDDKPEIEDVGsDEEEEEKK\t-- 68.8636\n",
    "* Peptide is composed of different amino acids (AA)\n",
    "* Each AA is indicated as a unique letter\n",
    "### Challenge:\n",
    "* Every AA introduces a different hydrophobicity <br>\n",
    "* Different order of the AAs makes them a different integrative hydrophobicity<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c630f-9a1e-4a47-aad0-59b195290c0b",
   "metadata": {},
   "source": [
    "## 1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b600c2-faae-4427-9ded-c59c48508bb2",
   "metadata": {},
   "source": [
    "### 1.1 General processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df8c256-443f-49ca-8780-27b099ae36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf604e3a-f0e3-4819-8142-454165b3737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('20231101_BMDM_phospho_PSMs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fedbb1-5110-4d6f-a5e9-62979ddd7979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0g/1v_dx9s92_d_fs88qdtmgffh0000gn/T/ipykernel_57587/750663411.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Annotated Sequence'] = df1['Annotated Sequence'].str.extract(r'\\.(.*?)\\.')\n",
      "/var/folders/0g/1v_dx9s92_d_fs88qdtmgffh0000gn/T/ipykernel_57587/750663411.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['length'] = df2['Annotated Sequence'].apply(len)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotated Sequence</th>\n",
       "      <th>Charge</th>\n",
       "      <th>m/z [Da]</th>\n",
       "      <th>MH+ [Da]</th>\n",
       "      <th>RT [min]</th>\n",
       "      <th>|Log Prob|</th>\n",
       "      <th>Byonic Score</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPSDYSNFDPEFLNEKPQLsFSDK</td>\n",
       "      <td>3</td>\n",
       "      <td>957.75176</td>\n",
       "      <td>2871.24073</td>\n",
       "      <td>123.1943</td>\n",
       "      <td>31.27</td>\n",
       "      <td>1612.2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESDDKPEIEDVGsDEEEEEKK</td>\n",
       "      <td>3</td>\n",
       "      <td>839.34072</td>\n",
       "      <td>2516.00761</td>\n",
       "      <td>68.8636</td>\n",
       "      <td>31.06</td>\n",
       "      <td>1591.8</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESDDKPEIEDVGsDEEEEEKK</td>\n",
       "      <td>3</td>\n",
       "      <td>839.34135</td>\n",
       "      <td>2516.00949</td>\n",
       "      <td>69.2675</td>\n",
       "      <td>31.06</td>\n",
       "      <td>1552.9</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Annotated Sequence  Charge   m/z [Da]    MH+ [Da]  RT [min]  \\\n",
       "0  SPSDYSNFDPEFLNEKPQLsFSDK       3  957.75176  2871.24073  123.1943   \n",
       "1     ESDDKPEIEDVGsDEEEEEKK       3  839.34072  2516.00761   68.8636   \n",
       "2     ESDDKPEIEDVGsDEEEEEKK       3  839.34135  2516.00949   69.2675   \n",
       "\n",
       "   |Log Prob|  Byonic Score  length  \n",
       "0       31.27        1612.2      24  \n",
       "1       31.06        1591.8      21  \n",
       "2       31.06        1552.9      21  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_columns = ['Annotated Sequence','Charge','m/z [Da]','MH+ [Da]','RT [min]','|Log Prob|','Byonic Score']\n",
    "df1= df[select_columns]\n",
    "# Remove data points with low confidences\n",
    "df2 = df1[(df1['Byonic Score'] > 500) & (df['|Log Prob|'] > 5)]\n",
    "df2['Annotated Sequence'] = df1['Annotated Sequence'].str.extract(r'\\.(.*?)\\.')\n",
    "feature=['Encoded']\n",
    "df2['length'] = df2['Annotated Sequence'].apply(len)\n",
    "df3 = df2[(df2['length'] > 8) & (df2['length'] <= 25)].reset_index(drop=True)\n",
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ddf83-04b1-458d-992b-f77eb6acbae2",
   "metadata": {},
   "source": [
    "### 1.2 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3af5a10d-9cd3-4181-becb-dbb2a878da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sequences = df3['Annotated Sequence'].values\n",
    "\n",
    "# Tokenize the sequences\n",
    "tokenizer = Tokenizer(char_level=True)  # Treat each character as a token\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "encoded_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "\n",
    "# Find the maximum length for padding\n",
    "max_length = max(len(seq) for seq in encoded_sequences)\n",
    "\n",
    "# Padding\n",
    "padded_sequences = pad_sequences(encoded_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "#Define the Embedding Layer parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704036dd-d87c-4f27-9557-7a58a650191e",
   "metadata": {},
   "source": [
    "### 1.3 Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fea05b1-d999-4421-b2e4-e4a77bf4366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def positional_encoding(max_length, embedding_dim):\n",
    "    PE = np.zeros((max_length, embedding_dim))\n",
    "    for pos in range(max_length):\n",
    "        for i in range(0, embedding_dim, 2):\n",
    "            PE[pos, i] = math.sin(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
    "            PE[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
    "    return PE\n",
    "\n",
    "pos_enc = positional_encoding(max_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b4b0e-0a2d-4395-b148-80a32433c35a",
   "metadata": {},
   "source": [
    "### 1.4 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "94928eb3-132e-40b1-bbd7-2c9634aa51f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18711, 25])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert padded sequences and retention times to PyTorch tensors\n",
    "X = torch.tensor(padded_sequences, dtype=torch.long)  # Long type for embeddings\n",
    "y = torch.tensor(df3['RT [min]'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Splitting the data into train, test, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=20)\n",
    "#X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=20)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "all_data = TensorDataset(X, y)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324a4534-93f6-4dbe-af10-fc584b494dd2",
   "metadata": {},
   "source": [
    "## 2. Model Buildup and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed712e1e-3a8f-4f17-a089-56edc32b8a71",
   "metadata": {},
   "source": [
    "### 2.1 Neuron Network with LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6f4699d0-ec39-46e3-86bd-30e0e6d7f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "embedding_dim = 30\n",
    "\n",
    "class PeptideLSTMNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_length):\n",
    "        super(PeptideLSTMNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(max_length, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=64, batch_first=True)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.out = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        pos_enc_tensor = torch.tensor(self.pos_encoding, dtype=torch.float32).to(x.device)\n",
    "        x = x + pos_enc_tensor[:x.size(1), :].unsqueeze(0)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Using the output of the last LSTM cell\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "model = PeptideLSTMNet(vocab_size=vocab_size, embedding_dim=embedding_dim, max_length=max_length)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6d7bc-c41f-4237-aea6-8e6abedc47d5",
   "metadata": {},
   "source": [
    "### 2.2 Four Layers -- Overfitting Model for Small-size Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "86ff8fc5-fffc-4f45-bf8c-1ec1fc5cfdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "embedding_dim = 30\n",
    "class PeptideLSTMNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_length):\n",
    "        super(PeptideLSTMNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(max_length, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=256, batch_first=True)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 24)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.bn4 = nn.BatchNorm1d(24)\n",
    "        self.fc4 = nn.Linear(24, 12)\n",
    "        self.out = nn.Linear(12, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        pos_enc_tensor = torch.tensor(self.pos_encoding, dtype=torch.float32).to(x.device)\n",
    "        x = x + pos_enc_tensor[:x.size(1), :].unsqueeze(0)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Using the output of the last LSTM cell\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.bn4(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "model = PeptideLSTMNet(vocab_size=vocab_size, embedding_dim=embedding_dim, max_length=max_length)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d41bd-c040-49f0-8497-ae6609de482c",
   "metadata": {},
   "source": [
    "### 2.3 Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1c9fabd7-6cdd-490c-be22-f2bb8eeb09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 0, Training Loss 4345.9150390625, Validation Loss 3829.972900390625\n",
      "Fold 1, Epoch 1, Training Loss 783.1192626953125\n",
      "Fold 1, Epoch 2, Training Loss 492.0649719238281\n",
      "Fold 1, Epoch 3, Training Loss 348.28961181640625\n",
      "Fold 1, Epoch 4, Training Loss 475.80987548828125\n",
      "Fold 1, Epoch 5, Training Loss 263.10418701171875\n",
      "Fold 1, Epoch 6, Training Loss 191.9562225341797\n",
      "Fold 1, Epoch 7, Training Loss 164.24026489257812\n",
      "Fold 1, Epoch 8, Training Loss 319.86810302734375\n",
      "Fold 1, Epoch 9, Training Loss 180.9126739501953\n",
      "Fold 1, Epoch 10, Training Loss 219.7364501953125\n",
      "Fold 1, Epoch 11, Training Loss 102.09355926513672\n",
      "Fold 1, Epoch 12, Training Loss 121.20842742919922\n",
      "Fold 1, Epoch 13, Training Loss 123.66524505615234\n",
      "Fold 1, Epoch 14, Training Loss 83.43072509765625\n",
      "Fold 1, Epoch 15, Training Loss 159.94760131835938\n",
      "Fold 1, Epoch 16, Training Loss 113.68753814697266\n",
      "Fold 1, Epoch 17, Training Loss 171.49937438964844\n",
      "Fold 1, Epoch 18, Training Loss 192.19642639160156\n",
      "Fold 1, Epoch 19, Training Loss 113.6396255493164\n",
      "Fold 1, Epoch 20, Training Loss 104.37263488769531, Validation Loss 90.71695709228516\n",
      "Fold 1, Epoch 21, Training Loss 150.747802734375\n",
      "Fold 1, Epoch 22, Training Loss 92.3189468383789\n",
      "Fold 1, Epoch 23, Training Loss 107.94206237792969\n",
      "Fold 1, Epoch 24, Training Loss 173.4984588623047\n",
      "Fold 1, Epoch 25, Training Loss 168.25625610351562\n",
      "Fold 1, Epoch 26, Training Loss 92.83663177490234\n",
      "Fold 1, Epoch 27, Training Loss 111.51924896240234\n",
      "Fold 1, Epoch 28, Training Loss 137.27066040039062\n",
      "Fold 1, Epoch 29, Training Loss 124.64273834228516\n",
      "Fold 1, Epoch 30, Training Loss 89.53697967529297\n",
      "Fold 1, Epoch 31, Training Loss 127.622314453125\n",
      "Fold 1, Epoch 32, Training Loss 96.52523040771484\n",
      "Fold 1, Epoch 33, Training Loss 123.21892547607422\n",
      "Fold 1, Epoch 34, Training Loss 114.55137634277344\n",
      "Fold 1, Epoch 35, Training Loss 103.72599792480469\n",
      "Fold 1, Epoch 36, Training Loss 84.72721099853516\n",
      "Fold 1, Epoch 37, Training Loss 101.46388244628906\n",
      "Fold 1, Epoch 38, Training Loss 141.73202514648438\n",
      "Fold 1, Epoch 39, Training Loss 124.90401458740234\n",
      "Fold 1, Epoch 40, Training Loss 107.65406036376953, Validation Loss 48.98985290527344\n",
      "Fold 1, Epoch 41, Training Loss 45.1228141784668\n",
      "Fold 1, Epoch 42, Training Loss 106.95589447021484\n",
      "Fold 1, Epoch 43, Training Loss 113.19023132324219\n",
      "Fold 1, Epoch 44, Training Loss 105.33079528808594\n",
      "Fold 1, Epoch 45, Training Loss 118.49494934082031\n",
      "Fold 1, Epoch 46, Training Loss 148.91371154785156\n",
      "Fold 1, Epoch 47, Training Loss 78.84589385986328\n",
      "Fold 1, Epoch 48, Training Loss 96.30631256103516\n",
      "Fold 1, Epoch 49, Training Loss 79.58503723144531\n",
      "Fold 1, Epoch 50, Training Loss 108.56919860839844\n",
      "Fold 2, Epoch 0, Training Loss 95.35443878173828, Validation Loss 97.29727935791016\n",
      "Fold 2, Epoch 1, Training Loss 86.6822280883789\n",
      "Fold 2, Epoch 2, Training Loss 83.51763916015625\n",
      "Fold 2, Epoch 3, Training Loss 110.81840515136719\n",
      "Fold 2, Epoch 4, Training Loss 123.84330749511719\n",
      "Fold 2, Epoch 5, Training Loss 104.85919189453125\n",
      "Fold 2, Epoch 6, Training Loss 91.49232482910156\n",
      "Fold 2, Epoch 7, Training Loss 77.55484008789062\n",
      "Fold 2, Epoch 8, Training Loss 90.00914764404297\n",
      "Fold 2, Epoch 9, Training Loss 140.14901733398438\n",
      "Fold 2, Epoch 10, Training Loss 142.58779907226562\n",
      "Fold 2, Epoch 11, Training Loss 86.00151062011719\n",
      "Fold 2, Epoch 12, Training Loss 65.75674438476562\n",
      "Fold 2, Epoch 13, Training Loss 106.63462829589844\n",
      "Fold 2, Epoch 14, Training Loss 139.42056274414062\n",
      "Fold 2, Epoch 15, Training Loss 56.42783737182617\n",
      "Fold 2, Epoch 16, Training Loss 77.6483383178711\n",
      "Fold 2, Epoch 17, Training Loss 70.8912582397461\n",
      "Fold 2, Epoch 18, Training Loss 132.48085021972656\n",
      "Fold 2, Epoch 19, Training Loss 44.91524124145508\n",
      "Fold 2, Epoch 20, Training Loss 121.31692504882812, Validation Loss 89.41349029541016\n",
      "Fold 2, Epoch 21, Training Loss 93.53691101074219\n",
      "Fold 2, Epoch 22, Training Loss 83.88870239257812\n",
      "Fold 2, Epoch 23, Training Loss 91.4886474609375\n",
      "Fold 2, Epoch 24, Training Loss 99.47369384765625\n",
      "Fold 2, Epoch 25, Training Loss 112.94810485839844\n",
      "Fold 2, Epoch 26, Training Loss 81.60234832763672\n",
      "Fold 2, Epoch 27, Training Loss 85.96309661865234\n",
      "Fold 2, Epoch 28, Training Loss 77.95275115966797\n",
      "Fold 2, Epoch 29, Training Loss 104.3696517944336\n",
      "Fold 2, Epoch 30, Training Loss 68.21006774902344\n",
      "Fold 2, Epoch 31, Training Loss 68.79822540283203\n",
      "Fold 2, Epoch 32, Training Loss 53.8128776550293\n",
      "Fold 2, Epoch 33, Training Loss 93.23321533203125\n",
      "Fold 2, Epoch 34, Training Loss 99.02217864990234\n",
      "Fold 2, Epoch 35, Training Loss 80.17711639404297\n",
      "Fold 2, Epoch 36, Training Loss 75.89398956298828\n",
      "Fold 2, Epoch 37, Training Loss 88.62229919433594\n",
      "Fold 2, Epoch 38, Training Loss 144.31275939941406\n",
      "Fold 2, Epoch 39, Training Loss 79.63363647460938\n",
      "Fold 2, Epoch 40, Training Loss 83.11583709716797, Validation Loss 44.7451057434082\n",
      "Fold 2, Epoch 41, Training Loss 101.60132598876953\n",
      "Fold 2, Epoch 42, Training Loss 78.14527130126953\n",
      "Fold 2, Epoch 43, Training Loss 137.2045135498047\n",
      "Fold 2, Epoch 44, Training Loss 96.78877258300781\n",
      "Fold 2, Epoch 45, Training Loss 123.19021606445312\n",
      "Fold 2, Epoch 46, Training Loss 81.0533447265625\n",
      "Fold 2, Epoch 47, Training Loss 94.7042236328125\n",
      "Fold 2, Epoch 48, Training Loss 110.6334457397461\n",
      "Fold 2, Epoch 49, Training Loss 97.64225006103516\n",
      "Fold 2, Epoch 50, Training Loss 85.57675170898438\n",
      "Fold 3, Epoch 0, Training Loss 88.85147094726562, Validation Loss 41.198509216308594\n",
      "Fold 3, Epoch 1, Training Loss 93.41931915283203\n",
      "Fold 3, Epoch 2, Training Loss 74.91727447509766\n",
      "Fold 3, Epoch 3, Training Loss 79.33271026611328\n",
      "Fold 3, Epoch 4, Training Loss 78.61088562011719\n",
      "Fold 3, Epoch 5, Training Loss 72.6978759765625\n",
      "Fold 3, Epoch 6, Training Loss 73.71981811523438\n",
      "Fold 3, Epoch 7, Training Loss 103.22040557861328\n",
      "Fold 3, Epoch 8, Training Loss 86.68948364257812\n",
      "Fold 3, Epoch 9, Training Loss 82.36198425292969\n",
      "Fold 3, Epoch 10, Training Loss 72.5885238647461\n",
      "Fold 3, Epoch 11, Training Loss 96.19107818603516\n",
      "Fold 3, Epoch 12, Training Loss 138.0889892578125\n",
      "Fold 3, Epoch 13, Training Loss 94.42204284667969\n",
      "Fold 3, Epoch 14, Training Loss 136.81056213378906\n",
      "Fold 3, Epoch 15, Training Loss 97.69959259033203\n",
      "Fold 3, Epoch 16, Training Loss 123.76953887939453\n",
      "Fold 3, Epoch 17, Training Loss 29.537111282348633\n",
      "Fold 3, Epoch 18, Training Loss 114.51993560791016\n",
      "Fold 3, Epoch 19, Training Loss 107.16400909423828\n",
      "Fold 3, Epoch 20, Training Loss 57.17704772949219, Validation Loss 40.19430160522461\n",
      "Fold 3, Epoch 21, Training Loss 93.33155822753906\n",
      "Fold 3, Epoch 22, Training Loss 100.79493713378906\n",
      "Fold 3, Epoch 23, Training Loss 113.67041778564453\n",
      "Fold 3, Epoch 24, Training Loss 89.44232940673828\n",
      "Fold 3, Epoch 25, Training Loss 68.85558319091797\n",
      "Fold 3, Epoch 26, Training Loss 89.8140869140625\n",
      "Fold 3, Epoch 27, Training Loss 82.60322570800781\n",
      "Fold 3, Epoch 28, Training Loss 92.6528091430664\n",
      "Fold 3, Epoch 29, Training Loss 51.09234619140625\n",
      "Fold 3, Epoch 30, Training Loss 118.61974334716797\n",
      "Fold 3, Epoch 31, Training Loss 81.34534454345703\n",
      "Fold 3, Epoch 32, Training Loss 82.13335418701172\n",
      "Fold 3, Epoch 33, Training Loss 84.04495239257812\n",
      "Fold 3, Epoch 34, Training Loss 107.57714080810547\n",
      "Fold 3, Epoch 35, Training Loss 123.84996795654297\n",
      "Fold 3, Epoch 36, Training Loss 101.37907409667969\n",
      "Fold 3, Epoch 37, Training Loss 117.04590606689453\n",
      "Fold 3, Epoch 38, Training Loss 101.40746307373047\n",
      "Fold 3, Epoch 39, Training Loss 88.25041198730469\n",
      "Fold 3, Epoch 40, Training Loss 55.07560729980469, Validation Loss 55.25770568847656\n",
      "Fold 3, Epoch 41, Training Loss 57.72303009033203\n",
      "Fold 3, Epoch 42, Training Loss 61.70947265625\n",
      "Fold 3, Epoch 43, Training Loss 68.58624267578125\n",
      "Fold 3, Epoch 44, Training Loss 82.02311706542969\n",
      "Fold 3, Epoch 45, Training Loss 91.12435150146484\n",
      "Fold 3, Epoch 46, Training Loss 69.92378234863281\n",
      "Fold 3, Epoch 47, Training Loss 70.32415771484375\n",
      "Fold 3, Epoch 48, Training Loss 87.23027038574219\n",
      "Fold 3, Epoch 49, Training Loss 66.09593963623047\n",
      "Fold 3, Epoch 50, Training Loss 99.94976806640625\n",
      "Fold 4, Epoch 0, Training Loss 70.027099609375, Validation Loss 63.0962028503418\n",
      "Fold 4, Epoch 1, Training Loss 78.60477447509766\n",
      "Fold 4, Epoch 2, Training Loss 98.75189971923828\n",
      "Fold 4, Epoch 3, Training Loss 72.3240966796875\n",
      "Fold 4, Epoch 4, Training Loss 75.72901153564453\n",
      "Fold 4, Epoch 5, Training Loss 97.64013671875\n",
      "Fold 4, Epoch 6, Training Loss 86.31830596923828\n",
      "Fold 4, Epoch 7, Training Loss 76.96158599853516\n",
      "Fold 4, Epoch 8, Training Loss 98.08756256103516\n",
      "Fold 4, Epoch 9, Training Loss 58.58742904663086\n",
      "Fold 4, Epoch 10, Training Loss 104.54820251464844\n",
      "Fold 4, Epoch 11, Training Loss 74.97040557861328\n",
      "Fold 4, Epoch 12, Training Loss 87.03767395019531\n",
      "Fold 4, Epoch 13, Training Loss 59.901851654052734\n",
      "Fold 4, Epoch 14, Training Loss 71.39949035644531\n",
      "Fold 4, Epoch 15, Training Loss 98.6224594116211\n",
      "Fold 4, Epoch 16, Training Loss 58.8987922668457\n",
      "Fold 4, Epoch 17, Training Loss 86.72759246826172\n",
      "Fold 4, Epoch 18, Training Loss 63.08540725708008\n",
      "Fold 4, Epoch 19, Training Loss 72.23535919189453\n",
      "Fold 4, Epoch 20, Training Loss 53.5180778503418, Validation Loss 27.81049919128418\n",
      "Fold 4, Epoch 21, Training Loss 124.83625793457031\n",
      "Fold 4, Epoch 22, Training Loss 79.73033142089844\n",
      "Fold 4, Epoch 23, Training Loss 102.76587677001953\n",
      "Fold 4, Epoch 24, Training Loss 85.88860321044922\n",
      "Fold 4, Epoch 25, Training Loss 84.39813232421875\n",
      "Fold 4, Epoch 26, Training Loss 139.0184326171875\n",
      "Fold 4, Epoch 27, Training Loss 72.95093536376953\n",
      "Fold 4, Epoch 28, Training Loss 76.60818481445312\n",
      "Fold 4, Epoch 29, Training Loss 88.09806823730469\n",
      "Fold 4, Epoch 30, Training Loss 86.53622436523438\n",
      "Fold 4, Epoch 31, Training Loss 73.40367889404297\n",
      "Fold 4, Epoch 32, Training Loss 86.89921569824219\n",
      "Fold 4, Epoch 33, Training Loss 93.78997039794922\n",
      "Fold 4, Epoch 34, Training Loss 115.02986907958984\n",
      "Fold 4, Epoch 35, Training Loss 87.72248840332031\n",
      "Fold 4, Epoch 36, Training Loss 74.84467315673828\n",
      "Fold 4, Epoch 37, Training Loss 80.4236068725586\n",
      "Fold 4, Epoch 38, Training Loss 129.92991638183594\n",
      "Fold 4, Epoch 39, Training Loss 108.48918151855469\n",
      "Fold 4, Epoch 40, Training Loss 72.19300079345703, Validation Loss 67.05023193359375\n",
      "Fold 4, Epoch 41, Training Loss 86.74095916748047\n",
      "Fold 4, Epoch 42, Training Loss 72.81087493896484\n",
      "Fold 4, Epoch 43, Training Loss 130.65133666992188\n",
      "Fold 4, Epoch 44, Training Loss 67.74230194091797\n",
      "Fold 4, Epoch 45, Training Loss 72.00914764404297\n",
      "Fold 4, Epoch 46, Training Loss 88.81873321533203\n",
      "Fold 4, Epoch 47, Training Loss 73.34912872314453\n",
      "Fold 4, Epoch 48, Training Loss 126.04076385498047\n",
      "Fold 4, Epoch 49, Training Loss 56.884437561035156\n",
      "Fold 4, Epoch 50, Training Loss 54.52665710449219\n",
      "Fold 5, Epoch 0, Training Loss 70.5086898803711, Validation Loss 14.637320518493652\n",
      "Fold 5, Epoch 1, Training Loss 89.99485778808594\n",
      "Fold 5, Epoch 2, Training Loss 71.70489501953125\n",
      "Fold 5, Epoch 3, Training Loss 94.12873840332031\n",
      "Fold 5, Epoch 4, Training Loss 70.51225280761719\n",
      "Fold 5, Epoch 5, Training Loss 66.1156997680664\n",
      "Fold 5, Epoch 6, Training Loss 80.50502014160156\n",
      "Fold 5, Epoch 7, Training Loss 70.3958511352539\n",
      "Fold 5, Epoch 8, Training Loss 97.26628112792969\n",
      "Fold 5, Epoch 9, Training Loss 73.60265350341797\n",
      "Fold 5, Epoch 10, Training Loss 78.73772430419922\n",
      "Fold 5, Epoch 11, Training Loss 40.44524383544922\n",
      "Fold 5, Epoch 12, Training Loss 79.07015991210938\n",
      "Fold 5, Epoch 13, Training Loss 83.4940185546875\n",
      "Fold 5, Epoch 14, Training Loss 63.509918212890625\n",
      "Fold 5, Epoch 15, Training Loss 52.942806243896484\n",
      "Fold 5, Epoch 16, Training Loss 63.243778228759766\n",
      "Fold 5, Epoch 17, Training Loss 77.60750579833984\n",
      "Fold 5, Epoch 18, Training Loss 67.91641998291016\n",
      "Fold 5, Epoch 19, Training Loss 130.7192840576172\n",
      "Fold 5, Epoch 20, Training Loss 85.28034973144531, Validation Loss 46.49969482421875\n",
      "Fold 5, Epoch 21, Training Loss 101.46891021728516\n",
      "Fold 5, Epoch 22, Training Loss 91.1955795288086\n",
      "Fold 5, Epoch 23, Training Loss 50.935455322265625\n",
      "Fold 5, Epoch 24, Training Loss 90.3603744506836\n",
      "Fold 5, Epoch 25, Training Loss 83.61783599853516\n",
      "Fold 5, Epoch 26, Training Loss 117.42638397216797\n",
      "Fold 5, Epoch 27, Training Loss 73.75856018066406\n",
      "Fold 5, Epoch 28, Training Loss 96.02592468261719\n",
      "Fold 5, Epoch 29, Training Loss 100.3553237915039\n",
      "Fold 5, Epoch 30, Training Loss 69.8292007446289\n",
      "Fold 5, Epoch 31, Training Loss 72.77310180664062\n",
      "Fold 5, Epoch 32, Training Loss 41.59745788574219\n",
      "Fold 5, Epoch 33, Training Loss 37.538360595703125\n",
      "Fold 5, Epoch 34, Training Loss 93.28141784667969\n",
      "Fold 5, Epoch 35, Training Loss 94.8901138305664\n",
      "Fold 5, Epoch 36, Training Loss 135.3199920654297\n",
      "Fold 5, Epoch 37, Training Loss 67.25882720947266\n",
      "Fold 5, Epoch 38, Training Loss 77.91349792480469\n",
      "Fold 5, Epoch 39, Training Loss 76.9523696899414\n",
      "Fold 5, Epoch 40, Training Loss 52.57023239135742, Validation Loss 28.82476043701172\n",
      "Fold 5, Epoch 41, Training Loss 77.86841583251953\n",
      "Fold 5, Epoch 42, Training Loss 64.18993377685547\n",
      "Fold 5, Epoch 43, Training Loss 74.0364761352539\n",
      "Fold 5, Epoch 44, Training Loss 126.20098876953125\n",
      "Fold 5, Epoch 45, Training Loss 105.14899444580078\n",
      "Fold 5, Epoch 46, Training Loss 86.73543548583984\n",
      "Fold 5, Epoch 47, Training Loss 68.47120666503906\n",
      "Fold 5, Epoch 48, Training Loss 83.1874771118164\n",
      "Fold 5, Epoch 49, Training Loss 86.52261352539062\n",
      "Fold 5, Epoch 50, Training Loss 63.336143493652344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# Use small data for hyperparameter tuning\n",
    "# dataset_size = len(test_data)\n",
    "dataset_size = len(train_data)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "history = []\n",
    "# Training loop function with cross-validation\n",
    "def train_model_with_cv(model, dataset, criterion, optimizer, epochs=50):\n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(indices)):\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "        fold_history = []\n",
    "        for epoch in range(epochs+1):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                model.eval()\n",
    "                #with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss = criterion(outputs, labels)\n",
    "                print('Fold {}, Epoch {}, Training Loss {}, Validation Loss {}'.format(fold+1, epoch, loss.item(), val_loss.item()))\n",
    "            else:\n",
    "                print('Fold {}, Epoch {}, Training Loss {}'.format(fold+1, epoch, loss.item()))\n",
    "            fold_history.append(loss.item())\n",
    "        history.append(fold_history)\n",
    "\n",
    "# Testing\n",
    "#train_model_with_cv(model, test_data, criterion, optimizer)\n",
    "train_model_with_cv(model, train_data, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ef8464d7-7234-429c-9161-0bc74bd40be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e57765ec-3919-4749-ae65-afa99f918bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.81873321533203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28df621d0>]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = []\n",
    "for i in history:\n",
    "    graph += i\n",
    "print(statistics.median(graph))\n",
    "plt.plot(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679397e9-64be-45bb-8c17-0c5c70d1309e",
   "metadata": {},
   "source": [
    "## 3. Prediction and Other Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb04bc5-26f5-4176-9840-c2fe61d3be41",
   "metadata": {},
   "source": [
    "### 3.1 Prediction of the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cf263517-5569-4671-a766-63a8824c25c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with 10 Tolerance: 86.54%\n",
      "Predictions: [63.663490295410156, 90.498291015625, 90.08969116210938, 87.46794891357422, 84.13177490234375]\n",
      "Actual Values: [64.45800018310547, 91.08429718017578, 112.83689880371094, 86.18910217285156, 80.87129974365234]\n"
     ]
    }
   ],
   "source": [
    "def predict_and_evaluate_accuracy(model, test_loader, tolerance=10.0):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predictions.extend(outputs.tolist())\n",
    "            actual_values.extend(labels.squeeze().tolist())\n",
    "\n",
    "            # Calculate accuracy within the tolerance range\n",
    "            correct_predictions += torch.sum(torch.abs(outputs - labels.squeeze()) <= tolerance)\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions.float() / total_predictions * 100\n",
    "\n",
    "    return predictions, actual_values, accuracy.item()\n",
    "\n",
    "predictions, actual_values, test_accuracy = predict_and_evaluate_accuracy(model, test_loader)\n",
    "print(f'Test Accuracy with 10 Tolerance: {test_accuracy:.2f}%')\n",
    "\n",
    "print(\"Predictions:\", predictions[0:5])\n",
    "print(\"Actual Values:\", actual_values[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309bd9e9-3ba2-4db9-8299-4130354a5f4f",
   "metadata": {},
   "source": [
    "### 3.2 Get Weight Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a9a8e0f5-d35e-4823-985c-c363f801c4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.2847e-01,  5.6858e-02, -4.5014e-01,  1.1169e+00, -2.3414e-01,\n",
      "         -1.0606e-01, -6.2777e-01, -3.6077e-01,  3.1041e-01,  3.7937e-01,\n",
      "          2.3753e-01,  1.2335e-01, -1.3020e-01,  9.8012e-02, -3.8509e-01,\n",
      "          4.0290e-02,  1.2544e-01, -2.7296e-01, -1.8250e-01,  1.2843e-01,\n",
      "          2.0874e-02, -1.2376e+00, -8.4315e-02,  5.0702e-02,  1.2950e-01,\n",
      "         -7.7868e-01, -2.0103e-01,  2.2111e-02,  1.5930e-01, -4.6583e-01],\n",
      "        [ 1.5314e+00,  3.0779e-01,  2.2975e-01, -8.0506e-01,  3.8078e-01,\n",
      "         -5.6286e-01, -1.8248e-01, -1.1479e+00,  2.0571e+00, -7.6001e-01,\n",
      "          2.4097e-02,  8.8095e-01, -2.0717e-01,  4.6575e-02, -8.9609e-01,\n",
      "          9.2091e-01, -9.2830e-01,  1.8499e-01, -2.5250e-01,  9.0252e-02,\n",
      "         -7.4355e-01, -1.7183e+00,  6.3870e-01, -2.9789e-01, -5.7779e-01,\n",
      "          6.4089e-01, -1.0215e+00, -2.0707e-01,  1.1305e+00, -1.7774e-01],\n",
      "        [ 3.1969e-01,  6.4029e-01, -2.1453e-01,  4.0081e-01, -3.4573e-01,\n",
      "          2.2901e-01,  3.1045e-01,  9.6111e-02,  2.2135e+00,  2.6987e-01,\n",
      "          4.3467e-01,  1.9193e-01, -3.1867e-01, -3.9022e-01,  1.3863e-01,\n",
      "         -1.2995e-01, -6.9675e-01,  1.2303e-01, -3.6241e-02,  5.0261e-01,\n",
      "          8.2393e-01,  1.0845e+00,  5.3706e-01,  6.8594e-01,  1.0874e+00,\n",
      "         -9.0299e-01, -6.3410e-01, -7.6234e-01,  1.4106e-01,  9.2472e-01],\n",
      "        [ 6.2350e-03,  5.3119e-01,  4.2143e-01, -4.7347e-01, -6.9775e-02,\n",
      "         -9.6212e-01, -1.8883e-01, -8.3074e-01,  2.8333e-01,  1.3286e-01,\n",
      "          2.1575e-01, -5.0248e-01, -1.0906e+00,  4.4257e-01,  9.2213e-02,\n",
      "         -8.9818e-02,  2.5851e-01, -5.1624e-01, -5.5992e-01, -1.5821e+00,\n",
      "          7.8518e-01,  6.8215e-01,  7.2318e-01, -2.8339e-01,  4.9296e-01,\n",
      "         -7.6414e-01, -9.3476e-01,  8.4306e-01,  1.7764e-02,  4.5595e-01],\n",
      "        [ 2.2440e-01,  4.8898e-01,  7.6634e-01,  1.9558e+00,  6.6455e-01,\n",
      "          6.2182e-01, -1.4639e+00, -4.5601e-01,  5.4746e-01,  8.7233e-01,\n",
      "          2.5375e-01,  4.6208e-01, -5.2478e-01,  3.8533e-01, -2.1167e+00,\n",
      "         -5.6388e-01, -1.4869e-01, -3.9431e-01, -1.5455e+00,  1.5978e-01,\n",
      "         -1.0885e-01, -1.1252e+00,  1.5789e+00,  1.8948e+00, -5.3812e-01,\n",
      "          4.7193e-01,  1.8672e+00, -3.4707e-01, -1.2090e+00,  7.6724e-02],\n",
      "        [-7.0446e-01,  6.9390e-01,  4.3733e-01,  1.6629e+00, -5.5050e-01,\n",
      "         -9.7992e-01, -7.1480e-01, -3.7940e-01, -2.1777e+00,  1.8618e-02,\n",
      "         -3.8910e-02,  8.9942e-01, -8.2598e-01,  1.1167e+00, -2.4084e-03,\n",
      "          2.0912e-01,  4.7771e-01, -5.5471e-01,  7.2630e-01,  3.5602e-01,\n",
      "          1.0523e+00,  7.0073e-01,  2.8938e-01,  1.5157e+00, -7.8598e-01,\n",
      "         -6.9890e-01, -1.3453e+00, -4.9875e-01,  1.8799e-01,  1.8921e+00],\n",
      "        [ 6.0514e-01,  9.0426e-01,  2.9181e-01, -7.1491e-02,  1.5497e-01,\n",
      "          4.7878e-01, -8.7928e-01, -9.9806e-01, -5.3257e-02, -1.2530e+00,\n",
      "         -1.4485e+00, -5.7500e-01, -6.0273e-02,  3.7153e-01, -7.5296e-01,\n",
      "         -6.5993e-02,  2.5367e-01,  4.3269e-01, -1.1559e+00, -1.3458e-01,\n",
      "          6.8605e-01, -5.6170e-01, -2.0353e-01, -3.7344e-01,  1.4932e-01,\n",
      "         -9.4562e-01,  1.1571e-01, -7.7578e-01, -6.5551e-01, -3.7041e-02],\n",
      "        [-8.2561e-01, -6.8021e-01, -5.8040e-01, -1.2860e+00,  1.1848e+00,\n",
      "          1.6277e+00, -6.3718e-02, -5.1221e-01,  2.7175e-01, -1.3082e-01,\n",
      "          4.5609e-01,  1.4755e+00,  2.7101e-01,  2.9349e-01,  4.4530e-01,\n",
      "          7.8984e-01,  8.8624e-01, -4.5685e-01,  5.0031e-01, -3.5038e-01,\n",
      "          4.5742e-01, -1.2381e+00, -1.1291e+00, -5.6687e-01, -8.9046e-01,\n",
      "         -1.0034e+00, -3.9855e-01, -1.0425e+00,  1.1410e-01, -1.7171e+00],\n",
      "        [ 2.2323e-01,  1.6156e+00, -1.1613e-01, -7.8869e-01,  3.9742e-01,\n",
      "          2.6088e-01,  1.1298e-01, -2.2681e-01, -1.2552e+00,  3.5067e-01,\n",
      "          1.4539e-01,  1.7506e-01,  1.2352e+00, -1.8621e-01, -7.8849e-01,\n",
      "         -4.9221e-01,  2.9906e-01, -4.9063e-02,  1.5249e+00,  1.2251e-01,\n",
      "         -6.6354e-01,  1.7491e+00, -1.7313e+00, -5.9492e-01,  1.4574e+00,\n",
      "          4.7971e-01,  2.9554e-01,  7.2699e-01, -5.7385e-01,  1.9755e-01],\n",
      "        [-3.8674e-01, -1.4688e+00, -1.0993e+00,  6.8059e-01,  1.0762e+00,\n",
      "          9.9005e-02, -2.2240e-01, -1.8155e-01,  6.4842e-01,  1.7146e-02,\n",
      "         -1.4864e+00, -3.1697e-01,  6.4791e-01,  3.8185e-01,  1.6328e+00,\n",
      "          1.3641e-01,  1.8309e-01, -9.4783e-01, -1.6739e+00, -3.2260e-01,\n",
      "         -5.2220e-01, -7.5117e-01,  5.3787e-01,  2.2590e+00,  1.2213e+00,\n",
      "         -9.2351e-01, -6.6268e-01,  1.5179e+00,  1.2815e+00,  7.0552e-01],\n",
      "        [-1.3227e+00, -1.3053e+00, -1.7470e-01, -4.8960e-02,  1.2627e+00,\n",
      "          1.7400e+00,  1.1020e+00, -1.5037e+00, -6.2926e-01, -8.9485e-01,\n",
      "          7.4253e-01, -2.6343e-01, -8.3333e-01, -5.2837e-01, -6.2153e-01,\n",
      "          2.9879e-02,  1.4993e-01, -1.4317e+00,  7.5434e-01, -6.6041e-01,\n",
      "          1.0179e+00, -1.5970e+00,  1.3568e-01, -7.0126e-01, -4.2035e-01,\n",
      "         -1.7286e-01, -2.8286e-01,  6.0359e-02,  5.2472e-01, -1.2003e+00],\n",
      "        [-6.2092e-01,  1.4670e+00, -8.7454e-01, -6.0294e-01, -1.1304e+00,\n",
      "          7.1872e-01,  8.4724e-01,  2.5712e-01, -4.4918e-01, -3.5653e-01,\n",
      "         -4.0233e-02,  7.3544e-02,  4.7519e-02, -1.5753e+00, -1.3000e+00,\n",
      "         -7.7472e-02, -3.5065e-01, -1.2071e+00, -1.1856e+00,  7.2631e-02,\n",
      "          1.9975e-01,  5.9454e-01,  4.2371e-01, -1.2885e+00, -2.0290e-02,\n",
      "          5.7271e-01,  1.0239e+00,  1.6174e+00,  7.9492e-01, -8.7751e-02],\n",
      "        [-1.5526e+00,  4.7449e-01, -1.1876e+00,  8.3435e-02, -3.0776e-01,\n",
      "         -5.3609e-01, -1.1219e+00, -2.7072e+00,  2.6452e-01,  1.7564e+00,\n",
      "          2.0386e-01, -1.0205e+00,  4.0591e-01, -3.1256e-01, -1.4168e-01,\n",
      "         -7.6066e-01,  2.0746e+00,  2.1792e+00,  3.4822e-01, -2.0907e-01,\n",
      "         -8.0309e-01, -5.6493e-01,  7.7617e-01, -4.1333e-01, -9.2230e-01,\n",
      "          1.1857e+00, -3.6827e-01,  7.3181e-01, -4.2880e-01, -5.9935e-01],\n",
      "        [-1.1396e+00, -9.8330e-01,  1.1486e+00, -1.3462e+00,  1.5889e+00,\n",
      "         -1.5953e+00, -5.8931e-01,  2.6082e-01,  6.5162e-02,  2.1234e+00,\n",
      "         -1.4891e+00,  9.8586e-02,  1.0399e+00,  1.6664e+00, -1.1232e+00,\n",
      "          5.6398e-01, -6.2424e-01, -3.2391e-01, -1.3546e+00,  4.3186e-01,\n",
      "         -6.2358e-01, -1.3047e+00, -2.0364e-01, -3.5097e-01, -3.0545e-01,\n",
      "         -1.1399e+00,  3.7013e-01, -7.1308e-01,  5.4499e-01,  8.9823e-02],\n",
      "        [ 1.0267e+00,  7.7635e-01,  1.8476e-01, -8.0520e-02,  9.4491e-01,\n",
      "          1.2761e-01, -1.3265e-01,  1.1815e+00, -4.8007e-01,  4.0493e-01,\n",
      "         -2.2069e-02, -1.5681e-01,  1.9248e+00, -6.5978e-01,  2.3829e+00,\n",
      "          9.1895e-01, -5.1390e-01,  1.3155e+00, -3.5341e-01,  8.3496e-01,\n",
      "         -1.1605e-01,  2.2513e-01, -4.9541e-01, -1.8563e-01,  7.9689e-03,\n",
      "         -9.2915e-01,  1.5037e+00,  1.2647e+00, -2.8368e-02, -1.3083e-01],\n",
      "        [ 2.6212e-01, -7.9760e-01,  1.1704e+00, -1.4576e-01, -9.1232e-01,\n",
      "          8.4031e-01,  1.3549e+00,  7.4651e-02, -1.0536e+00,  9.7876e-02,\n",
      "         -5.6971e-01, -6.3303e-01,  1.1929e+00,  3.9129e-01,  1.2399e+00,\n",
      "         -8.8960e-02, -1.4875e+00, -8.1136e-01,  3.1467e-01, -1.4242e-01,\n",
      "          2.5689e-01,  8.0316e-01, -1.4010e+00, -7.8653e-01,  1.3697e+00,\n",
      "          8.5477e-01,  6.9626e-01,  4.1874e-01, -2.8939e+00, -3.3058e-01],\n",
      "        [-1.9310e+00, -1.5080e+00,  1.4387e+00,  1.2951e+00,  1.3689e+00,\n",
      "         -1.3670e-01,  7.7101e-01,  4.3579e-01,  2.2703e+00, -9.2564e-01,\n",
      "          1.0718e+00,  3.7110e+00,  3.6686e-01,  2.7848e+00,  4.4357e-01,\n",
      "          5.2208e-02,  5.1966e-01, -4.6109e-01,  1.5357e-01,  1.0916e+00,\n",
      "         -3.5113e-01, -1.6819e-01,  4.6670e-01, -1.3519e+00, -1.8045e+00,\n",
      "         -1.2783e+00,  7.3854e-01,  3.9220e+00,  1.2188e+00, -1.2008e+00],\n",
      "        [ 2.0227e+00, -6.1509e-01, -1.0527e+00,  1.1012e+00,  7.6556e-01,\n",
      "         -9.8194e-01,  1.0184e+00, -1.2329e-01, -7.8049e-01, -4.0535e-01,\n",
      "         -7.1640e-02, -1.3142e-01, -6.1041e-02, -1.3468e+00,  2.7994e-01,\n",
      "          5.8017e-02,  7.6644e-01,  3.4148e-01, -8.2021e-01,  5.8297e-01,\n",
      "          2.4525e-01,  5.6846e-01, -7.0182e-01, -5.8878e-01,  4.5214e-01,\n",
      "          1.3287e+00,  5.4868e-01, -2.2809e-01, -2.1977e-01, -3.1794e-01],\n",
      "        [ 3.1530e-01,  9.7146e-01,  1.9107e+00, -1.0320e+00, -2.2292e+00,\n",
      "          9.9051e-01,  7.9243e-01,  2.5209e-01,  5.8530e-01,  5.6304e-02,\n",
      "         -4.7503e-01, -4.8659e-01,  2.6495e+00, -2.9164e+00, -1.2181e+00,\n",
      "          1.8104e-01,  1.0654e+00,  5.1704e-02, -7.8548e-02,  2.0137e+00,\n",
      "         -1.3820e+00, -5.5926e-01,  1.2283e+00, -1.7537e-01,  1.7628e-01,\n",
      "          9.9235e-01, -1.4029e+00, -3.5427e-02, -7.1414e-01,  5.6454e-01],\n",
      "        [ 2.7762e-01, -6.1677e-01,  7.9821e-01,  1.8732e+00,  1.0208e+00,\n",
      "         -7.5450e-01,  1.0166e+00,  8.9627e-01,  2.7324e-01,  3.3208e-02,\n",
      "         -7.3042e-01, -1.6926e+00, -2.4377e+00,  7.0155e-01, -1.1029e+00,\n",
      "         -3.6865e-01,  7.0847e-01,  2.0303e+00,  7.4601e-01,  2.7509e-01,\n",
      "          6.3910e-01,  8.3264e-02,  1.1611e+00,  6.3483e-01,  3.7222e-01,\n",
      "         -6.3887e-01,  8.0110e-01,  5.1565e-01, -2.5729e-01, -1.8141e+00],\n",
      "        [ 5.3835e-01, -3.6019e-01,  7.4545e-01, -1.6662e+00, -1.6916e+00,\n",
      "          8.4262e-01,  3.3093e+00, -4.3290e-01, -1.6176e+00, -1.0621e+00,\n",
      "          1.4293e+00,  4.1465e-01, -8.3500e-01,  6.6862e-02,  2.0243e+00,\n",
      "          7.5245e-01,  4.0513e-01,  1.4269e+00,  9.5055e-01,  1.1440e+00,\n",
      "         -2.4854e-01,  2.7273e-01, -5.8840e-01,  5.5917e-02,  1.2252e+00,\n",
      "          2.0526e+00,  3.2089e+00,  3.4789e+00, -2.0988e+00, -4.1161e-01]])\n",
      "tensor([[ 0.5264, -0.6872,  0.2615,  ...,  0.3595, -0.4475,  0.3723],\n",
      "        [ 0.1625, -0.0044,  0.4459,  ...,  0.2912, -0.2748,  0.0216],\n",
      "        [ 0.0830, -0.0376, -0.1841,  ...,  0.3267, -0.4206, -0.0015],\n",
      "        ...,\n",
      "        [ 0.5506, -0.0305,  0.3317,  ...,  0.5926,  0.4899,  0.0846],\n",
      "        [ 0.7617,  0.3245,  0.6706,  ...,  0.6708,  0.0762,  0.1677],\n",
      "        [ 0.2597,  0.6754,  0.5680,  ..., -0.0394, -0.1147,  0.4664]])\n",
      "tensor([[ 0.0693,  0.4864,  0.1249,  ..., -0.5020,  0.8652, -0.4472],\n",
      "        [-0.4286, -0.3131,  0.1934,  ..., -0.7622,  0.4725,  0.1169],\n",
      "        [ 0.2206, -0.1652,  0.2690,  ...,  0.0091,  0.2523, -0.3500],\n",
      "        ...,\n",
      "        [ 0.2257, -0.3491, -0.1559,  ..., -0.3759, -0.5149, -1.3529],\n",
      "        [ 0.1175, -1.2871, -0.1684,  ...,  0.2746, -0.1413, -0.7859],\n",
      "        [-0.6174, -1.0256, -0.2165,  ...,  0.7352, -1.0309,  0.1356]])\n",
      "tensor([-6.0921e-02, -9.8535e-01, -4.7895e-01, -2.3624e-01, -3.9321e-01,\n",
      "         1.8854e-01, -9.8756e-02, -1.3897e-01,  5.4956e-01, -2.4810e-01,\n",
      "         9.9026e-02, -1.8827e-01,  6.5889e-02, -2.1055e-01, -4.1062e-01,\n",
      "        -4.3150e-02,  1.9388e-01, -4.9176e-01, -1.3586e-01,  1.6881e-01,\n",
      "        -1.2941e-02,  4.9262e-01, -8.2602e-01,  1.2544e-01, -3.8126e-01,\n",
      "        -2.7609e-01, -1.3166e-01, -3.4123e-01, -4.4495e-01, -1.2804e-03,\n",
      "        -3.4051e-01,  3.7528e-01, -7.6845e-01, -5.7652e-01, -1.4427e-01,\n",
      "         5.3239e-02, -1.0512e-02,  1.3329e-01, -1.9927e-01, -2.2742e-02,\n",
      "        -2.6192e-01, -5.6513e-01, -4.4505e-01,  5.7516e-02, -3.9565e-01,\n",
      "        -2.8698e-01,  1.3748e-01,  2.6669e-01, -9.4258e-02, -4.1213e-01,\n",
      "         1.0652e-01,  7.0980e-02, -3.1342e-01, -2.5106e-01,  2.3559e-01,\n",
      "        -9.8643e-02, -4.1665e-01, -4.8158e-02, -6.8294e-02,  2.3476e-01,\n",
      "         3.2235e-01, -1.6635e-01,  3.7183e-01,  2.8748e-01,  3.1452e-02,\n",
      "         1.1557e-01,  3.7066e-01,  1.6259e-01, -2.2654e-01,  3.4932e-01,\n",
      "         3.2149e-01,  1.3479e-01,  5.3788e-01,  2.6232e-01,  3.3118e-01,\n",
      "        -4.1328e-01,  2.9986e-01,  4.2212e-01,  1.6715e-01,  3.0975e-01,\n",
      "        -9.4022e-02,  9.4455e-03,  3.4693e-01, -1.0380e-01, -2.7346e-01,\n",
      "        -2.6920e-01,  4.6953e-01, -2.2764e-01, -4.1428e-01, -1.8545e-01,\n",
      "         2.6679e-01, -6.7294e-02,  9.2663e-02,  7.7782e-02,  8.6489e-01,\n",
      "         3.9908e-01,  5.6301e-01,  4.2390e-01, -2.0580e-02, -3.6094e-02,\n",
      "        -1.6042e-01,  1.9336e-01, -1.3181e-01, -1.8027e-01, -2.3955e-01,\n",
      "         1.0760e-01, -5.2391e-02,  3.4213e-01,  1.6829e-01,  1.7251e-01,\n",
      "         4.5480e-01,  1.5440e-02,  6.0978e-02,  4.8688e-01, -3.9335e-02,\n",
      "        -1.0182e-02,  4.7696e-01, -2.2415e-01,  2.4291e-01, -1.3790e-01,\n",
      "         5.5612e-01, -2.8821e-01,  1.7684e-01, -2.5666e-01,  1.8768e-01,\n",
      "         8.3662e-01, -1.3639e-01,  2.2041e-01, -1.4997e-01, -4.3344e-01,\n",
      "        -1.0066e-01, -2.4114e-01, -1.3098e-02,  5.3893e-02, -4.9736e-01,\n",
      "         3.6928e-02, -1.8459e-01,  5.4299e-03,  1.6954e-01, -7.8482e-02,\n",
      "         3.6606e-01,  2.3116e-01,  7.2201e-02,  2.2544e-01,  1.9915e-01,\n",
      "         5.5906e-03, -1.2700e-01,  1.0762e-01, -2.2652e-01, -1.8443e-01,\n",
      "         1.9021e-01,  2.9183e-01,  3.5177e-01, -1.1341e-02, -2.1733e-01,\n",
      "         1.7068e-01, -1.6778e-01,  1.5984e-01, -4.6495e-01, -8.5717e-02,\n",
      "        -1.9504e-01,  1.4211e-01, -3.3336e-02, -3.2951e-01,  2.2105e-01,\n",
      "         1.1338e-01,  7.5361e-03, -9.5645e-02, -5.2047e-01, -3.7895e-01,\n",
      "        -5.6409e-01,  1.1900e-02, -2.8033e-01, -1.0949e-01, -5.0202e-02,\n",
      "        -2.7450e-01,  5.4365e-01, -1.6307e-02,  5.5614e-02,  5.5622e-02,\n",
      "         1.9222e-01, -4.8398e-02, -2.8444e-01,  2.5513e-01, -2.9009e-01,\n",
      "         2.2347e-02, -4.2282e-03,  3.4360e-01,  2.9786e-01,  1.7796e-01,\n",
      "        -7.2655e-02, -4.4813e-02, -5.2269e-01,  2.7833e-02,  1.3390e-01,\n",
      "         1.3644e-02, -2.2747e-01,  1.5015e-01, -1.2170e-01,  3.9775e-01,\n",
      "        -1.7688e-02, -1.8014e-01, -1.6051e-01, -2.3550e-02,  1.4022e-01,\n",
      "         4.0027e-01,  4.5712e-03, -6.8798e-02, -1.8679e-02,  3.1802e-01,\n",
      "        -6.3828e-01, -5.7400e-02,  1.7068e-01,  2.8733e-01,  1.5681e-01,\n",
      "        -1.0376e-01, -4.6221e-01, -5.7882e-01, -2.9694e-01, -9.9549e-02,\n",
      "         2.9387e-01, -7.3135e-04,  2.5202e-01, -6.5140e-01, -1.1092e-01,\n",
      "        -3.0973e-01,  3.1720e-01, -2.9881e-01, -5.5355e-03, -4.4118e-01,\n",
      "        -4.8548e-02, -1.2428e-01, -9.9989e-02,  1.2688e-02,  2.1499e-01,\n",
      "         2.5937e-01,  1.2844e-01,  9.7502e-02, -3.7146e-01,  2.0256e-01,\n",
      "        -1.4777e-02,  1.5913e-01, -4.9041e-02,  3.4847e-01, -4.5004e-02,\n",
      "        -2.8028e-01, -3.3782e-02, -1.4318e-01,  2.6533e-02, -5.8389e-01,\n",
      "         2.0755e-01,  1.0250e-01, -4.7842e-01, -4.1101e-01,  2.2098e-01,\n",
      "         1.1954e-01])\n",
      "tensor([ 6.9101e-02, -9.9521e-01, -3.1676e-01, -1.9102e-01, -3.1462e-01,\n",
      "         2.4594e-01, -2.0822e-02,  2.0566e-02,  5.0377e-01, -3.3604e-01,\n",
      "         1.6774e-01, -3.4578e-01,  4.9091e-02, -1.6383e-01, -3.7692e-01,\n",
      "        -1.6751e-01,  2.5718e-01, -4.1291e-01,  1.3690e-02,  4.4629e-02,\n",
      "        -2.7340e-03,  5.2433e-01, -7.6196e-01, -7.5911e-02, -3.3995e-01,\n",
      "        -3.5350e-01, -4.4911e-02, -3.9720e-01, -3.9024e-01,  1.2931e-01,\n",
      "        -3.9599e-01,  2.9850e-01, -8.1350e-01, -6.3692e-01, -3.0686e-01,\n",
      "         1.8755e-01, -6.0945e-02,  1.9403e-01, -7.0868e-02,  1.0776e-01,\n",
      "        -9.8104e-02, -5.3529e-01, -4.2581e-01, -6.2520e-02, -4.9833e-01,\n",
      "        -2.6240e-01,  4.2423e-02,  2.2517e-01,  2.8544e-02, -3.7205e-01,\n",
      "        -3.9769e-02,  7.2273e-02, -2.9862e-01, -1.4617e-01,  3.8388e-01,\n",
      "        -2.9191e-01, -5.8108e-01, -1.8936e-01, -2.0731e-01,  2.4803e-01,\n",
      "         1.9626e-01, -3.3595e-01,  4.2745e-01,  2.7499e-01, -5.7640e-02,\n",
      "         1.7630e-01,  3.7036e-01,  3.8059e-01, -1.6440e-01,  4.0375e-01,\n",
      "         3.5916e-01,  1.5802e-01,  5.4513e-01,  3.0229e-01,  2.8281e-01,\n",
      "        -2.9105e-01,  4.0213e-01,  3.7310e-01,  2.8081e-01,  4.1836e-01,\n",
      "         1.3302e-01,  1.3706e-03,  4.9307e-01, -7.6001e-02, -2.6115e-01,\n",
      "        -6.9551e-02,  5.5332e-01, -2.8290e-01, -3.4226e-01, -2.2060e-01,\n",
      "         1.3046e-01, -5.4404e-02, -2.8339e-02,  1.1578e-01,  8.5377e-01,\n",
      "         3.8555e-01,  5.6855e-01,  3.3311e-01, -1.5292e-01,  2.6165e-02,\n",
      "        -2.5483e-01,  1.0434e-01, -1.6618e-01, -3.3079e-01, -8.1833e-02,\n",
      "         3.5608e-02, -1.4677e-01,  4.3025e-01,  2.9032e-01, -7.4135e-03,\n",
      "         3.4619e-01, -1.3460e-01, -5.2235e-02,  4.0588e-01,  2.8719e-02,\n",
      "         3.7015e-02,  4.8455e-01, -3.8486e-01,  1.4271e-01,  1.7577e-02,\n",
      "         4.1521e-01, -5.1422e-01,  5.3790e-02, -1.2016e-01,  1.1908e-01,\n",
      "         6.7270e-01, -1.6756e-01,  2.0322e-01, -2.0804e-01, -5.0827e-01,\n",
      "        -5.9546e-02, -3.9345e-01,  2.1684e-01,  1.7101e-01, -4.1533e-01,\n",
      "         6.3765e-02, -1.1634e-01, -8.6026e-03,  2.7519e-01, -6.2546e-02,\n",
      "         4.1245e-01,  1.8721e-01,  1.6328e-01,  1.3533e-01,  2.1285e-01,\n",
      "        -7.7593e-02, -2.8389e-01, -4.7324e-02, -2.2344e-01, -6.8267e-02,\n",
      "         1.9837e-01,  3.0377e-01,  2.9384e-01, -9.7110e-03, -1.6449e-01,\n",
      "         6.6055e-02, -4.0387e-02,  1.2170e-01, -5.3273e-01,  1.2840e-01,\n",
      "        -3.5557e-01, -3.0625e-02,  8.7937e-03, -2.9587e-01,  1.7206e-01,\n",
      "         3.5152e-02,  1.6828e-01, -3.7159e-02, -5.5015e-01, -4.6506e-01,\n",
      "        -5.3447e-01, -9.8439e-02, -4.0050e-01, -6.2377e-02, -7.5643e-02,\n",
      "        -3.1461e-01,  4.8069e-01, -6.0329e-02,  3.3250e-02,  9.9854e-02,\n",
      "         7.7559e-02,  1.2175e-02, -2.4613e-01,  2.8229e-01, -1.8827e-01,\n",
      "         8.8089e-04,  5.2566e-02,  2.3293e-01,  1.6898e-01,  3.1649e-01,\n",
      "        -1.1008e-01, -5.8808e-02, -5.6982e-01,  1.9055e-01,  7.5461e-02,\n",
      "         2.9826e-02, -4.3709e-01,  4.2218e-02, -1.4331e-02,  4.6506e-01,\n",
      "        -1.9582e-01, -2.0091e-01, -8.0082e-02, -1.3218e-01,  1.4570e-01,\n",
      "         3.1565e-01,  6.6118e-02, -4.4779e-03,  2.4756e-02,  2.5542e-01,\n",
      "        -5.6667e-01, -5.8814e-02,  2.1718e-01,  1.4814e-01,  2.0912e-01,\n",
      "        -1.6054e-01, -5.4976e-01, -6.8675e-01, -3.7684e-01, -6.5205e-02,\n",
      "         1.5301e-01,  1.0188e-01,  2.0524e-01, -7.8387e-01, -7.5959e-02,\n",
      "        -2.4455e-01,  3.2508e-01, -1.4579e-01,  9.3694e-03, -3.9232e-01,\n",
      "        -8.0344e-02, -2.1683e-01, -1.6177e-01, -5.5501e-03, -1.2153e-02,\n",
      "         1.9136e-01, -9.7987e-02,  1.5999e-02, -3.2498e-01,  1.3259e-01,\n",
      "        -8.8741e-02,  7.7073e-02, -2.4859e-01,  3.6818e-01,  1.9101e-02,\n",
      "        -1.3335e-01, -3.4441e-02, -2.2714e-01,  2.0637e-01, -4.5572e-01,\n",
      "         1.6375e-01,  1.1404e-01, -3.6046e-01, -3.9242e-01,  1.5139e-01,\n",
      "         1.6062e-01])\n",
      "tensor([ 0.7835,  1.4634,  0.8618, -0.0346,  0.4824, -0.0341,  1.1971,  0.5254,\n",
      "         0.2282,  0.4316, -0.0417,  1.1405,  0.9077,  0.8076,  1.2244,  0.8511,\n",
      "        -0.0218,  0.8603,  1.1952, -0.2003,  0.4049,  0.3356,  1.1471,  1.3367,\n",
      "         2.5143,  1.3155,  1.0901,  0.3094,  1.0733,  0.0098,  0.6597,  0.5328,\n",
      "         1.5421,  1.3541,  0.1610,  0.2801, -0.0994, -0.0279,  0.7201,  1.7982,\n",
      "         0.0330,  1.0139,  1.7726,  0.1402,  0.5426,  0.8255,  0.6655,  0.8911,\n",
      "         0.4028,  1.2017,  0.0801,  0.3348,  1.3853,  0.6988,  0.0797,  1.1367,\n",
      "         1.0343,  1.6085,  1.0418,  0.7230,  0.2727,  1.0193,  0.9453,  0.2401])\n",
      "tensor([ 0.2086,  0.7207,  0.9638, -0.0690, -0.5488,  0.0016,  0.4152, -0.0115,\n",
      "        -0.0288,  0.3185, -0.0410, -0.2814, -0.3463,  0.1934, -0.9027,  0.2581,\n",
      "        -0.0111,  0.4834,  0.3826, -0.0781, -0.1832, -0.1155,  0.2025, -0.1288,\n",
      "         0.0540,  0.7266,  0.3083, -0.3256,  0.2432,  0.0062,  0.0722,  0.3535,\n",
      "         0.7029,  0.8179,  0.0016, -0.2763,  0.0259, -0.0171, -0.4889,  0.0642,\n",
      "        -0.0349,  0.8516, -0.5955,  0.0580, -0.8578,  0.6844, -0.2637,  0.1353,\n",
      "        -0.1768,  0.3254,  0.0302, -0.1087,  0.8450,  0.2490, -0.0685,  0.3424,\n",
      "        -0.0856,  0.1125,  0.3316,  0.2173,  0.2308,  0.5286,  0.5154, -0.1015])\n",
      "tensor([[-0.1848, -0.0777, -0.1028,  ..., -0.0263, -0.2846, -0.0537],\n",
      "        [ 0.2996,  0.3315,  0.1175,  ..., -0.6162,  0.2962,  0.0978],\n",
      "        [ 0.0254,  0.7619,  0.7082,  ..., -0.5495, -0.1790,  0.0676],\n",
      "        ...,\n",
      "        [ 0.3228,  0.6495,  0.1791,  ...,  0.0959,  0.4392, -0.4314],\n",
      "        [ 0.2997,  0.6846,  0.6235,  ...,  0.7530,  0.5352, -0.0128],\n",
      "        [ 0.7975,  0.4430,  0.8072,  ...,  0.6045, -0.7927, -0.1718]])\n",
      "tensor([-0.6608, -0.2809,  0.1859, -0.0698,  0.1258, -0.3657,  0.6220, -0.3228,\n",
      "        -0.1569, -0.0364,  0.1173, -0.5233, -0.0315, -0.5354, -0.5445, -0.0581,\n",
      "         0.6448,  0.5038, -0.2331, -0.0903, -0.4036, -0.4021, -0.1906, -0.3353,\n",
      "         0.1175,  0.2769,  0.5583, -0.6739, -0.0988, -0.2030,  0.3913,  0.2870])\n",
      "tensor([0.8142, 1.7360, 0.8784, 1.4194, 1.3085, 0.8794, 1.3816, 1.7278, 1.7774,\n",
      "        1.5277, 1.5747, 0.6569, 1.0696, 0.8934, 0.4395, 1.6375, 1.5776, 1.5008,\n",
      "        1.0128, 1.5183, 1.3011, 1.0187, 1.0631, 1.6904, 0.9226, 1.2305, 1.4739,\n",
      "        0.7729, 1.7343, 1.7173, 1.3822, 1.2091])\n",
      "tensor([ 0.0108,  0.3539,  0.2176,  0.4307,  0.3470, -0.0806,  0.3932,  0.3957,\n",
      "         0.3531,  0.5140,  0.5210, -0.0242,  0.3529,  0.0257, -0.0432,  0.5084,\n",
      "         0.1688,  0.2991,  0.2182,  0.4408,  0.1031,  0.0151,  0.1395,  0.3489,\n",
      "         0.1722,  0.4329,  0.3643, -0.0803,  0.3785,  0.4035,  0.4291,  0.3229])\n",
      "tensor([[-6.8242e-02, -8.8708e-02,  2.2907e-01, -2.6799e-01, -2.0338e-01,\n",
      "          2.9714e-02, -4.1104e-01, -1.1967e-01, -1.9258e-01, -2.6571e-01,\n",
      "         -3.3924e-01,  5.7990e-02, -4.4691e-01, -3.7664e-02,  1.7121e-01,\n",
      "         -3.1290e-01, -3.1815e-01, -3.0269e-01,  1.1407e-01, -2.2954e-01,\n",
      "         -1.0644e-01, -3.7780e-03, -1.1975e-01, -2.1019e-01,  2.1810e-01,\n",
      "         -5.4179e-01, -4.2712e-01,  5.2816e-01, -1.3958e-01, -2.3578e-01,\n",
      "         -3.6093e-01, -2.3353e-01],\n",
      "        [-1.0692e-01, -1.1942e+00, -2.0747e-01, -4.3147e-01,  7.5378e-02,\n",
      "          1.7662e-01, -1.0289e-01, -1.6936e+00, -1.5150e+00, -6.1548e-01,\n",
      "         -4.3410e-01,  1.0413e-01, -2.0214e-01,  4.5132e-01, -1.7512e-02,\n",
      "         -5.5947e-01, -6.3550e-02,  2.5248e-02, -3.0194e-02, -1.5424e+00,\n",
      "         -5.1888e-01, -8.2894e-02, -9.7043e-01, -1.1356e+00, -2.9299e-01,\n",
      "         -1.2745e-01, -1.3907e-01,  8.8231e-01, -1.1268e+00, -6.6948e-01,\n",
      "         -1.5737e-01,  8.8253e-02],\n",
      "        [-9.2100e-02,  4.1476e-01,  4.8912e-01,  2.7655e-01,  4.1719e-01,\n",
      "          7.9961e-02,  3.1977e-01,  3.0253e-01,  2.5635e-01,  1.8120e-01,\n",
      "          2.2813e-01, -3.4724e-02, -3.7128e-01, -4.2633e-01,  1.5978e+00,\n",
      "          2.9258e-01,  2.4964e-01,  3.8736e-01, -6.4183e+00,  2.0489e-01,\n",
      "          5.1954e-01, -2.1360e-01,  5.2380e-01,  3.0419e-01,  5.0401e-01,\n",
      "          2.8338e-01,  3.1795e-01,  1.9664e-01,  3.0009e-01,  2.8148e-01,\n",
      "          2.3438e-01,  4.2581e-01],\n",
      "        [-9.7123e-02, -2.1651e-01,  4.3265e-01, -8.8409e-01, -1.6887e-01,\n",
      "         -2.4925e-02, -3.7176e-01, -3.4291e-01, -3.1460e-01, -5.7855e-01,\n",
      "         -7.3152e-01,  2.6094e-02, -3.6538e-01, -8.4483e-02, -2.2635e-02,\n",
      "         -5.4418e-01, -2.4899e-01, -2.6405e-01,  1.8879e-01, -3.2873e-01,\n",
      "         -1.1867e-01,  4.4206e-02, -3.3468e-01, -2.0559e-01,  3.1382e-01,\n",
      "         -4.5383e-01, -3.3314e-01,  4.4907e-01, -1.8785e-01, -5.3354e-01,\n",
      "         -3.2324e-01, -1.4524e-01],\n",
      "        [-1.2739e-01, -4.1114e-01, -1.9998e+00, -8.5785e-02,  3.4460e-01,\n",
      "         -2.1969e-02,  2.8780e-02, -2.2179e-01, -2.8021e-01, -1.1678e-01,\n",
      "         -4.0283e-02, -1.9711e-02, -3.6383e-02,  1.8737e-01, -2.4435e+00,\n",
      "         -8.3376e-02,  4.2120e-02,  8.8348e-02, -7.1950e-01, -2.1254e-01,\n",
      "         -7.3306e-01, -8.5826e-02, -1.6502e+00, -4.9888e-01, -2.0042e+00,\n",
      "         -1.1725e-02,  7.2322e-03,  6.2122e-02, -4.7538e-01, -1.3908e-01,\n",
      "         -2.8834e-02,  3.3972e-01],\n",
      "        [-1.4760e-01,  3.6228e-01,  6.9429e-01,  2.5873e-01,  5.4161e-01,\n",
      "         -2.3004e-01,  2.7010e-01,  2.7655e-01,  2.1929e-01,  2.6242e-01,\n",
      "          2.3325e-01,  1.9493e-01, -1.3749e+00, -4.2583e-01,  1.5895e+00,\n",
      "          3.2316e-01,  3.2207e-01,  3.3984e-01, -3.5938e+00,  3.4455e-01,\n",
      "          4.2274e-01, -1.2594e-01,  4.1517e-01,  3.1522e-01,  4.9841e-01,\n",
      "          3.6643e-01,  2.5023e-01,  5.1577e-01,  2.9301e-01,  2.5121e-01,\n",
      "          1.9858e-01,  5.0818e-01],\n",
      "        [-3.9331e-02, -5.7466e-01, -1.2002e+00,  1.4183e-02,  2.3107e-01,\n",
      "          3.1580e-02,  1.1196e-01, -3.2358e-01, -3.1338e-01, -8.8177e-02,\n",
      "         -1.5151e-02,  7.1016e-02, -7.0315e-03,  4.5233e-02, -1.1593e+00,\n",
      "         -7.0552e-02,  7.9911e-02,  1.1885e-01, -3.7730e-01, -1.7521e-01,\n",
      "         -5.3079e-01,  6.1130e-02, -1.1965e+00, -8.6848e-01, -1.2325e+00,\n",
      "          9.1330e-02,  9.4771e-02,  6.0990e-01, -5.4473e-01, -8.0057e-02,\n",
      "          3.1386e-02,  2.3531e-01],\n",
      "        [ 1.4888e-01, -1.5420e+00, -1.1980e+00, -1.7476e-01,  3.2214e-01,\n",
      "         -5.2855e-02, -3.4300e-03, -8.6830e-01, -8.0677e-01, -2.2673e-01,\n",
      "         -1.7958e-01, -2.0482e-01, -8.1809e-02,  2.7321e-01, -1.0592e+00,\n",
      "         -1.3922e-01,  2.7817e-02,  1.1813e-01, -4.0074e-01, -4.6940e-01,\n",
      "         -8.8958e-01, -2.2148e-01, -1.6700e+00, -1.5309e+00, -1.2163e+00,\n",
      "         -3.9408e-02, -1.9927e-02,  2.2570e-01, -1.4500e+00, -2.7371e-01,\n",
      "         -2.8424e-02,  3.5446e-01],\n",
      "        [ 1.6874e-01,  2.9267e-01,  6.0753e-01,  2.8034e-01,  5.2915e-01,\n",
      "         -8.4041e-02,  3.0873e-01,  3.3918e-01,  2.5550e-01,  2.8427e-01,\n",
      "          2.1894e-01, -4.7711e-01, -1.1217e+00, -3.4800e-01,  1.5785e+00,\n",
      "          2.1786e-01,  2.9714e-01,  3.4140e-01, -4.3966e+00,  3.7879e-01,\n",
      "          3.9768e-01, -4.8991e-01,  6.1161e-01,  3.5095e-01,  4.4030e-01,\n",
      "          2.4284e-01,  2.7790e-01,  5.0070e-01,  2.7009e-01,  2.6436e-01,\n",
      "          3.1471e-01,  4.5318e-01],\n",
      "        [-1.1898e-01,  3.3968e-01,  5.4574e-01,  3.0322e-01,  4.5996e-01,\n",
      "          1.5191e-01,  2.8668e-01,  2.7928e-01,  2.5981e-01,  3.5933e-01,\n",
      "          2.7119e-01, -1.8943e-01, -8.2153e-01, -3.8308e-01,  1.7721e+00,\n",
      "          3.0795e-01,  2.9194e-01,  3.9500e-01, -5.0546e+00,  3.5460e-01,\n",
      "          2.9892e-01, -5.6632e-01,  6.5214e-01,  2.4652e-01,  5.4321e-01,\n",
      "          3.1241e-01,  1.9208e-01,  3.7698e-01,  3.7793e-01,  2.6178e-01,\n",
      "          2.3617e-01,  5.2277e-01],\n",
      "        [ 2.1421e-02, -3.5886e-01,  4.3605e-01, -7.8816e-01, -5.0796e-02,\n",
      "          5.5832e-02, -1.6760e-01, -4.2176e-01, -4.8056e-01, -7.8525e-01,\n",
      "         -6.6900e-01,  1.4457e-02, -2.0660e-01, -6.1588e-02, -2.9576e-01,\n",
      "         -8.2725e-01, -1.3676e-01, -9.9635e-02,  2.4665e-01, -6.3703e-01,\n",
      "         -2.7215e-01, -2.8127e-02, -3.5831e-01, -3.7799e-01,  4.0236e-01,\n",
      "         -2.4319e-01, -2.4044e-01,  5.3674e-01, -3.0838e-01, -7.8718e-01,\n",
      "         -1.9752e-01, -7.0867e-02],\n",
      "        [ 2.1984e-03, -1.8495e-01,  9.6613e-02, -1.8604e-01, -1.9676e-01,\n",
      "          1.8410e-01, -4.1605e-01, -2.9855e-01, -2.0960e-01, -1.8858e-01,\n",
      "         -1.3317e-01,  1.8153e-03, -3.2229e-01, -1.6602e-02,  3.5713e-01,\n",
      "         -1.4493e-01, -2.5702e-01, -2.7697e-01,  8.3328e-02, -2.8570e-01,\n",
      "         -6.0427e-02, -4.7801e-02, -2.2809e-02, -1.6984e-01,  1.7458e-01,\n",
      "         -3.6222e-01, -3.4222e-01,  6.2828e-01, -1.4682e-01, -1.2814e-01,\n",
      "         -4.7108e-01, -2.0218e-01],\n",
      "        [-5.0062e-01,  3.8531e-01,  6.0224e-01,  3.4573e-01,  5.5025e-01,\n",
      "         -1.3574e-01,  2.8466e-01,  3.9154e-01,  2.5392e-01,  3.1188e-01,\n",
      "          2.7519e-01, -3.6024e-01, -9.1285e-01, -2.7930e-01,  1.7342e+00,\n",
      "          2.0441e-01,  2.3687e-01,  3.6851e-01, -4.8287e+00,  3.8786e-01,\n",
      "          5.8457e-01, -1.9447e-01,  5.6369e-01,  3.3511e-01,  5.7255e-01,\n",
      "          3.8747e-01,  2.6604e-01,  1.3857e-01,  3.5342e-01,  2.2247e-01,\n",
      "          3.1652e-01,  6.0814e-01],\n",
      "        [-6.5340e-02, -9.1095e-02,  1.4299e-01, -2.4645e-01, -2.3644e-01,\n",
      "          1.0620e-01, -4.0736e-01, -1.3476e-01, -8.1089e-02, -1.9561e-01,\n",
      "         -1.8140e-01,  9.7061e-02, -3.7585e-01, -1.7932e-01,  1.8520e-01,\n",
      "         -1.9148e-01, -2.6616e-01, -3.0328e-01,  3.2904e-02, -1.0277e-01,\n",
      "         -4.8578e-02, -3.7051e-02, -1.0649e-01, -5.6535e-02,  1.2410e-01,\n",
      "         -5.1287e-01, -3.7442e-01,  6.0583e-01, -8.0554e-02, -1.3529e-01,\n",
      "         -4.0462e-01, -2.5138e-01],\n",
      "        [-3.2624e-02, -4.1480e-01,  2.9715e-01, -5.7609e-01, -2.1880e-02,\n",
      "          3.3459e-02, -1.1861e-01, -6.7549e-01, -6.8632e-01, -1.0261e+00,\n",
      "         -5.8087e-01,  7.1168e-02, -1.7069e-01,  1.3273e-01, -2.8555e-01,\n",
      "         -7.6175e-01, -5.1862e-02, -3.5657e-02,  2.0378e-01, -1.0595e+00,\n",
      "         -2.8740e-01,  5.3338e-02, -3.8848e-01, -4.2749e-01,  3.2160e-01,\n",
      "         -1.9384e-01, -1.8204e-01,  5.5654e-01, -4.6824e-01, -8.6854e-01,\n",
      "         -1.4665e-01,  3.8260e-02],\n",
      "        [ 5.9804e-02,  2.7056e-01,  5.4576e-01,  2.4021e-01,  3.8894e-01,\n",
      "         -1.6092e-01,  2.7235e-01,  2.5506e-01,  2.4648e-01,  3.7613e-01,\n",
      "          2.5142e-01,  8.8433e-02, -4.5072e-01, -5.0695e-01,  1.7014e+00,\n",
      "          2.5080e-01,  2.2851e-01,  3.7653e-01, -6.0271e+00,  3.3272e-01,\n",
      "          5.6743e-01,  1.5358e-02,  4.9854e-01,  2.5523e-01,  4.5063e-01,\n",
      "          3.4440e-01,  2.8039e-01,  1.7030e-01,  2.3048e-01,  1.8932e-01,\n",
      "          2.8099e-01,  4.5470e-01]])\n",
      "tensor([-1.0389, -0.8405, -0.0288, -0.9595, -0.2488, -0.3670, -0.9276, -0.4139,\n",
      "        -0.2125, -0.1881, -0.8513, -1.1814, -0.2670, -1.2431, -0.7498, -0.2028])\n",
      "tensor([2.3586, 2.2519, 0.4286, 2.4387, 1.8043, 0.5194, 2.3227, 1.9726, 0.4945,\n",
      "        0.4767, 2.4286, 2.6490, 0.4876, 2.4325, 2.2490, 0.4137])\n",
      "tensor([-0.1344, -0.1050,  1.6440, -0.2035, -0.0980,  1.4240,  0.0088, -0.1113,\n",
      "         1.5638,  1.6228, -0.1100, -0.3336,  1.6083, -0.2255, -0.4184,  1.8635])\n",
      "tensor([[-2.0624, -0.1901,  0.2665, -1.3154, -0.9129,  0.3567, -0.7799, -0.5532,\n",
      "          0.5085,  0.6217, -0.7905, -2.1596,  0.5027, -2.1545, -0.3933,  0.4429],\n",
      "        [-0.1516, -0.6098,  1.2504, -0.1814, -0.5180,  1.2504, -0.2565, -0.5639,\n",
      "          1.2056,  1.2276, -0.1995, -0.1059,  0.6442, -0.1388, -0.4576,  0.9710],\n",
      "        [-0.1806, -0.5243,  1.3856, -0.2424, -0.5388,  1.0414, -0.5455, -0.5950,\n",
      "          0.9757,  1.2958, -0.3575, -0.1463,  1.2329, -0.2029, -0.2424,  1.2861],\n",
      "        [ 0.0034, -0.0135,  0.0243, -0.0075, -0.0048, -0.0504, -0.0307,  0.0068,\n",
      "          0.0151, -0.0822,  0.0072,  0.0048,  0.0230, -0.0377,  0.0109, -0.0840],\n",
      "        [ 0.2154,  1.4189,  0.5704,  0.3309,  1.6811, -0.3768,  1.9091,  1.7472,\n",
      "         -0.1987, -0.0114,  0.4325,  0.1888, -0.0461,  0.2336,  0.5985,  0.7491],\n",
      "        [-0.3754, -0.6676,  1.3915, -0.2014, -0.3689,  1.0073, -0.5759, -0.5603,\n",
      "          1.2270,  1.0634, -0.1924, -0.4089,  1.1961, -0.3903, -0.2667,  1.3867],\n",
      "        [-1.3372, -1.0963, -0.4089, -1.7730, -0.6205, -0.3548, -0.6379, -0.6981,\n",
      "         -0.5283, -0.6526, -1.9381, -1.1895, -0.4901, -1.2942, -1.7942, -0.4640],\n",
      "        [-0.1402, -0.5922,  0.9172, -0.1530, -0.0990,  0.6537, -0.6163, -0.4140,\n",
      "          0.9159,  0.8945, -0.2344, -0.1177,  1.1386, -0.1366, -0.3379,  1.1526]])\n",
      "tensor([ 0.4676,  1.2720,  1.1783, -0.3984,  3.9279,  1.2617, -0.4522,  1.1990])\n",
      "tensor([[ 1.0359,  0.8936,  1.0636, -0.2043,  1.6792,  0.9469,  1.6925,  0.6745]])\n",
      "tensor([0.2779])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af5a7e-1161-47d7-a4d0-1397f971df5c",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47a2d7-bcc1-4dbe-9f4f-52be2d2f45a4",
   "metadata": {},
   "source": [
    "* Our model has a average loss of 70, pretty comparable to the published literature value (loss: 33~133)\n",
    "* Tolerance +- 10 is sufficient enough for the application of setting the dynamic window for a targeted peptide on the instrument (mass spectrometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a4d02-2b09-427c-823d-570bb1a4350b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
