{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d9f672-e3f7-42f0-a4ad-abfb34d88731",
   "metadata": {},
   "source": [
    "# Sequential Peptide Retention Time Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c8de7-2148-476b-9481-92a64fd246a0",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "* Sequence to value -- Peptide sequence to a time point (0~180 min)\n",
    "### Background:\n",
    "* ESDDKPEIEDVGsDEEEEEKK\t-- 68.8636\n",
    "* Peptide is composed of different amino acids (AA)\n",
    "* Each AA is indicated as a unique letter\n",
    "### Challenge:\n",
    "* Every AA introduces a different hydrophobicity <br>\n",
    "* Different order of the AAs makes them a different integrative hydrophobicity<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041c630f-9a1e-4a47-aad0-59b195290c0b",
   "metadata": {},
   "source": [
    "## 1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b600c2-faae-4427-9ded-c59c48508bb2",
   "metadata": {},
   "source": [
    "### 1.1 General processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df8c256-443f-49ca-8780-27b099ae36aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf604e3a-f0e3-4819-8142-454165b3737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('20231101_BMDM_phospho_PSMs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9ab2a-e79f-4628-b3f8-33c7befec7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_columns = ['Annotated Sequence','Charge','m/z [Da]','MH+ [Da]','RT [min]','|Log Prob|','Byonic Score']\n",
    "df1= df[select_columns]\n",
    "# Remove data points with low confidences\n",
    "df2 = df1[(df1['Byonic Score'] > 500) & (df['|Log Prob|'] > 5)]\n",
    "df2['Annotated Sequence'] = df1['Annotated Sequence'].str.extract(r'\\.(.*?)\\.')\n",
    "feature=['Encoded']\n",
    "df2['length'] = df2['Annotated Sequence'].apply(len)\n",
    "df3 = df2[(df2['length'] > 8) & (df2['length'] <= 25)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c05c304e-fd4c-4551-87c5-c546dfd85713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotated Sequence</th>\n",
       "      <th>Charge</th>\n",
       "      <th>m/z [Da]</th>\n",
       "      <th>MH+ [Da]</th>\n",
       "      <th>RT [min]</th>\n",
       "      <th>|Log Prob|</th>\n",
       "      <th>Byonic Score</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SPSDYSNFDPEFLNEKPQLsFSDK</td>\n",
       "      <td>3</td>\n",
       "      <td>957.75176</td>\n",
       "      <td>2871.24073</td>\n",
       "      <td>123.1943</td>\n",
       "      <td>31.27</td>\n",
       "      <td>1612.2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ESDDKPEIEDVGsDEEEEEKK</td>\n",
       "      <td>3</td>\n",
       "      <td>839.34072</td>\n",
       "      <td>2516.00761</td>\n",
       "      <td>68.8636</td>\n",
       "      <td>31.06</td>\n",
       "      <td>1591.8</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ESDDKPEIEDVGsDEEEEEKK</td>\n",
       "      <td>3</td>\n",
       "      <td>839.34135</td>\n",
       "      <td>2516.00949</td>\n",
       "      <td>69.2675</td>\n",
       "      <td>31.06</td>\n",
       "      <td>1552.9</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EGINPGYDDYADsDEDQHDAYLER</td>\n",
       "      <td>3</td>\n",
       "      <td>956.36950</td>\n",
       "      <td>2867.09394</td>\n",
       "      <td>93.6957</td>\n",
       "      <td>30.59</td>\n",
       "      <td>1508.7</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SPSDYSNFDPEFLNEKPQLsFSDK</td>\n",
       "      <td>3</td>\n",
       "      <td>957.75184</td>\n",
       "      <td>2871.24098</td>\n",
       "      <td>123.3552</td>\n",
       "      <td>30.59</td>\n",
       "      <td>1522.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Annotated Sequence  Charge   m/z [Da]    MH+ [Da]  RT [min]  \\\n",
       "0  SPSDYSNFDPEFLNEKPQLsFSDK       3  957.75176  2871.24073  123.1943   \n",
       "1     ESDDKPEIEDVGsDEEEEEKK       3  839.34072  2516.00761   68.8636   \n",
       "2     ESDDKPEIEDVGsDEEEEEKK       3  839.34135  2516.00949   69.2675   \n",
       "3  EGINPGYDDYADsDEDQHDAYLER       3  956.36950  2867.09394   93.6957   \n",
       "4  SPSDYSNFDPEFLNEKPQLsFSDK       3  957.75184  2871.24098  123.3552   \n",
       "\n",
       "   |Log Prob|  Byonic Score  length  \n",
       "0       31.27        1612.2      24  \n",
       "1       31.06        1591.8      21  \n",
       "2       31.06        1552.9      21  \n",
       "3       30.59        1508.7      24  \n",
       "4       30.59        1522.0      24  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca477e-fd05-469d-8a91-f2db8e87c4c2",
   "metadata": {},
   "source": [
    "Take average of the time under same peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44475e15-c2f9-4771-81df-d5dcb397a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3.groupby('Annotated Sequence')['RT [min]'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0c92062-613f-42ae-a08a-4b34f09dd257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Annotated Sequence</th>\n",
       "      <th>RT [min]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAADDGEEPKsEPETK</td>\n",
       "      <td>41.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAATPEsQEPQAK</td>\n",
       "      <td>43.417550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAtPESQEPQAK</td>\n",
       "      <td>46.447017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAtPESQEPQAk</td>\n",
       "      <td>42.193767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAtPEsQEPQAK</td>\n",
       "      <td>40.368500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Annotated Sequence   RT [min]\n",
       "0   AAADDGEEPKsEPETK  41.828600\n",
       "1      AAATPEsQEPQAK  43.417550\n",
       "2      AAAtPESQEPQAK  46.447017\n",
       "3      AAAtPESQEPQAk  42.193767\n",
       "4      AAAtPEsQEPQAK  40.368500"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ddf83-04b1-458d-992b-f77eb6acbae2",
   "metadata": {},
   "source": [
    "### 1.2 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3af5a10d-9cd3-4181-becb-dbb2a878da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sequences = df4['Annotated Sequence'].values\n",
    "\n",
    "# Tokenize the sequences\n",
    "tokenizer = Tokenizer(char_level=True)  # Treat each character as a token\n",
    "tokenizer.fit_on_texts(sequences)\n",
    "encoded_sequences = tokenizer.texts_to_sequences(sequences)\n",
    "\n",
    "# Find the maximum length for padding\n",
    "max_length = max(len(seq) for seq in encoded_sequences)\n",
    "\n",
    "# Padding\n",
    "padded_sequences = pad_sequences(encoded_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "#Define the Embedding Layer parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704036dd-d87c-4f27-9557-7a58a650191e",
   "metadata": {},
   "source": [
    "### 1.3 Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fea05b1-d999-4421-b2e4-e4a77bf4366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "embedding_dim = 30\n",
    "def positional_encoding(max_length, embedding_dim):\n",
    "    PE = np.zeros((max_length, embedding_dim))\n",
    "    for pos in range(max_length):\n",
    "        for i in range(0, embedding_dim, 2):\n",
    "            PE[pos, i] = math.sin(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
    "            PE[pos, i + 1] = math.cos(pos / (10000 ** ((2 * i) / embedding_dim)))\n",
    "    return PE\n",
    "\n",
    "pos_enc = positional_encoding(max_length, embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b4b0e-0a2d-4395-b148-80a32433c35a",
   "metadata": {},
   "source": [
    "### 1.4 Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94928eb3-132e-40b1-bbd7-2c9634aa51f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3618, 25])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "\n",
    "random.seed(21)\n",
    "# Convert padded sequences and retention times to PyTorch tensors\n",
    "X = torch.tensor(padded_sequences, dtype=torch.long)  # Long type for embeddings\n",
    "y = torch.tensor(df4['RT [min]'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Splitting the data into train, test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "all_data = TensorDataset(X, y)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324a4534-93f6-4dbe-af10-fc584b494dd2",
   "metadata": {},
   "source": [
    "## 2. Model Buildup and Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed712e1e-3a8f-4f17-a089-56edc32b8a71",
   "metadata": {},
   "source": [
    "### 2.1 Neuron Network with LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6f4699d0-ec39-46e3-86bd-30e0e6d7f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class PeptideLSTMNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_length):\n",
    "        super(PeptideLSTMNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        #self.pos_encoding = positional_encoding(max_length, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=64, batch_first=True)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.fc3 = nn.Linear(16, 8)\n",
    "        self.out = nn.Linear(8, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        #pos_enc_tensor = torch.tensor(self.pos_encoding, dtype=torch.float32).to(x.device)\n",
    "        #x = x + pos_enc_tensor[:x.size(1), :].unsqueeze(0)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Using the output of the last LSTM cell\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "model = PeptideLSTMNet(vocab_size=vocab_size, embedding_dim=embedding_dim, max_length=max_length)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6d7bc-c41f-4237-aea6-8e6abedc47d5",
   "metadata": {},
   "source": [
    "### 2.2 Four Layers -- Overfitting Model for Small-size Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "86ff8fc5-fffc-4f45-bf8c-1ec1fc5cfdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "embedding_dim = 30\n",
    "class PeptideLSTMNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, max_length):\n",
    "        super(PeptideLSTMNet, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.pos_encoding = positional_encoding(max_length, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=256, batch_first=True)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.fc3 = nn.Linear(64, 24)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.bn4 = nn.BatchNorm1d(24)\n",
    "        self.fc4 = nn.Linear(24, 12)\n",
    "        self.out = nn.Linear(12, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        pos_enc_tensor = torch.tensor(self.pos_encoding, dtype=torch.float32).to(x.device)\n",
    "        x = x + pos_enc_tensor[:x.size(1), :].unsqueeze(0)\n",
    "        \n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Using the output of the last LSTM cell\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.bn4(x)\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "model = PeptideLSTMNet(vocab_size=vocab_size, embedding_dim=embedding_dim, max_length=max_length)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d41bd-c040-49f0-8497-ae6609de482c",
   "metadata": {},
   "source": [
    "### 2.3 Training with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1c9fabd7-6cdd-490c-be22-f2bb8eeb09dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, Epoch 0, Training Loss 6850.50244140625, Validation Loss 6933.93310546875\n",
      "Fold 1, Epoch 1, Training Loss 6638.81982421875\n",
      "Fold 1, Epoch 2, Training Loss 5970.81640625\n",
      "Fold 1, Epoch 3, Training Loss 6103.84326171875\n",
      "Fold 1, Epoch 4, Training Loss 6757.728515625\n",
      "Fold 1, Epoch 5, Training Loss 6086.4541015625\n",
      "Fold 1, Epoch 6, Training Loss 6700.4521484375\n",
      "Fold 1, Epoch 7, Training Loss 5801.43017578125\n",
      "Fold 1, Epoch 8, Training Loss 5639.05908203125\n",
      "Fold 1, Epoch 9, Training Loss 5123.4443359375\n",
      "Fold 1, Epoch 10, Training Loss 5833.458984375\n",
      "Fold 1, Epoch 11, Training Loss 5327.9921875\n",
      "Fold 1, Epoch 12, Training Loss 4662.396484375\n",
      "Fold 1, Epoch 13, Training Loss 4412.24462890625\n",
      "Fold 1, Epoch 14, Training Loss 3683.2392578125\n",
      "Fold 1, Epoch 15, Training Loss 3670.090576171875\n",
      "Fold 1, Epoch 16, Training Loss 3377.57080078125\n",
      "Fold 1, Epoch 17, Training Loss 2665.505126953125\n",
      "Fold 1, Epoch 18, Training Loss 2639.23681640625\n",
      "Fold 1, Epoch 19, Training Loss 1584.3489990234375\n",
      "Fold 1, Epoch 20, Training Loss 2003.98779296875, Validation Loss 1595.4908447265625\n",
      "Fold 1, Epoch 21, Training Loss 1393.282470703125\n",
      "Fold 1, Epoch 22, Training Loss 1562.05126953125\n",
      "Fold 1, Epoch 23, Training Loss 1289.7987060546875\n",
      "Fold 1, Epoch 24, Training Loss 919.6156616210938\n",
      "Fold 1, Epoch 25, Training Loss 600.674072265625\n",
      "Fold 1, Epoch 26, Training Loss 500.0549011230469\n",
      "Fold 1, Epoch 27, Training Loss 522.182861328125\n",
      "Fold 1, Epoch 28, Training Loss 446.6081848144531\n",
      "Fold 1, Epoch 29, Training Loss 390.8487548828125\n",
      "Fold 1, Epoch 30, Training Loss 526.5106201171875\n",
      "Fold 1, Epoch 31, Training Loss 298.1830749511719\n",
      "Fold 1, Epoch 32, Training Loss 286.2952880859375\n",
      "Fold 1, Epoch 33, Training Loss 189.3440704345703\n",
      "Fold 1, Epoch 34, Training Loss 192.32122802734375\n",
      "Fold 1, Epoch 35, Training Loss 233.5714111328125\n",
      "Fold 1, Epoch 36, Training Loss 509.8845520019531\n",
      "Fold 1, Epoch 37, Training Loss 222.54652404785156\n",
      "Fold 1, Epoch 38, Training Loss 245.51043701171875\n",
      "Fold 1, Epoch 39, Training Loss 511.58758544921875\n",
      "Fold 1, Epoch 40, Training Loss 252.1106719970703, Validation Loss 144.6089630126953\n",
      "Fold 1, Epoch 41, Training Loss 179.77305603027344\n",
      "Fold 1, Epoch 42, Training Loss 318.5357971191406\n",
      "Fold 1, Epoch 43, Training Loss 450.6943664550781\n",
      "Fold 1, Epoch 44, Training Loss 207.61703491210938\n",
      "Fold 1, Epoch 45, Training Loss 301.81524658203125\n",
      "Fold 1, Epoch 46, Training Loss 207.7506561279297\n",
      "Fold 1, Epoch 47, Training Loss 210.1549530029297\n",
      "Fold 1, Epoch 48, Training Loss 274.3157043457031\n",
      "Fold 1, Epoch 49, Training Loss 316.7760314941406\n",
      "Fold 1, Epoch 50, Training Loss 284.1927490234375\n",
      "Fold 1, Epoch 51, Training Loss 383.8878173828125\n",
      "Fold 1, Epoch 52, Training Loss 299.83099365234375\n",
      "Fold 1, Epoch 53, Training Loss 514.236328125\n",
      "Fold 1, Epoch 54, Training Loss 139.577392578125\n",
      "Fold 1, Epoch 55, Training Loss 175.84461975097656\n",
      "Fold 1, Epoch 56, Training Loss 264.910400390625\n",
      "Fold 1, Epoch 57, Training Loss 204.8640899658203\n",
      "Fold 1, Epoch 58, Training Loss 158.88685607910156\n",
      "Fold 1, Epoch 59, Training Loss 256.13653564453125\n",
      "Fold 1, Epoch 60, Training Loss 175.90817260742188, Validation Loss 69.73282623291016\n",
      "Fold 1, Epoch 61, Training Loss 240.27008056640625\n",
      "Fold 1, Epoch 62, Training Loss 194.91778564453125\n",
      "Fold 1, Epoch 63, Training Loss 210.30270385742188\n",
      "Fold 1, Epoch 64, Training Loss 112.4454345703125\n",
      "Fold 1, Epoch 65, Training Loss 295.70263671875\n",
      "Fold 1, Epoch 66, Training Loss 116.5843734741211\n",
      "Fold 1, Epoch 67, Training Loss 216.3369140625\n",
      "Fold 1, Epoch 68, Training Loss 235.05027770996094\n",
      "Fold 1, Epoch 69, Training Loss 411.7394714355469\n",
      "Fold 1, Epoch 70, Training Loss 222.33731079101562\n",
      "Fold 1, Epoch 71, Training Loss 161.5807342529297\n",
      "Fold 1, Epoch 72, Training Loss 196.5657501220703\n",
      "Fold 1, Epoch 73, Training Loss 120.25127410888672\n",
      "Fold 1, Epoch 74, Training Loss 102.91401672363281\n",
      "Fold 1, Epoch 75, Training Loss 184.24221801757812\n",
      "Fold 1, Epoch 76, Training Loss 133.60777282714844\n",
      "Fold 1, Epoch 77, Training Loss 173.8282928466797\n",
      "Fold 1, Epoch 78, Training Loss 126.326171875\n",
      "Fold 1, Epoch 79, Training Loss 167.0380096435547\n",
      "Fold 1, Epoch 80, Training Loss 138.8226318359375, Validation Loss 83.76445007324219\n",
      "Fold 1, Epoch 81, Training Loss 105.31320190429688\n",
      "Fold 1, Epoch 82, Training Loss 145.18565368652344\n",
      "Fold 1, Epoch 83, Training Loss 232.03335571289062\n",
      "Fold 1, Epoch 84, Training Loss 130.06216430664062\n",
      "Fold 1, Epoch 85, Training Loss 123.80955505371094\n",
      "Fold 1, Epoch 86, Training Loss 295.5817565917969\n",
      "Fold 1, Epoch 87, Training Loss 188.10292053222656\n",
      "Fold 1, Epoch 88, Training Loss 192.19378662109375\n",
      "Fold 1, Epoch 89, Training Loss 155.89407348632812\n",
      "Fold 1, Epoch 90, Training Loss 174.57449340820312\n",
      "Fold 1, Epoch 91, Training Loss 164.95375061035156\n",
      "Fold 1, Epoch 92, Training Loss 130.6055450439453\n",
      "Fold 1, Epoch 93, Training Loss 101.67626190185547\n",
      "Fold 1, Epoch 94, Training Loss 196.8247528076172\n",
      "Fold 1, Epoch 95, Training Loss 160.65444946289062\n",
      "Fold 1, Epoch 96, Training Loss 114.66230773925781\n",
      "Fold 1, Epoch 97, Training Loss 138.6004638671875\n",
      "Fold 1, Epoch 98, Training Loss 267.6619873046875\n",
      "Fold 1, Epoch 99, Training Loss 244.69715881347656\n",
      "Fold 1, Epoch 100, Training Loss 131.9121856689453, Validation Loss 62.483734130859375\n",
      "Fold 2, Epoch 0, Training Loss 137.16888427734375, Validation Loss 47.722991943359375\n",
      "Fold 2, Epoch 1, Training Loss 148.31253051757812\n",
      "Fold 2, Epoch 2, Training Loss 130.05670166015625\n",
      "Fold 2, Epoch 3, Training Loss 108.08953094482422\n",
      "Fold 2, Epoch 4, Training Loss 163.4711456298828\n",
      "Fold 2, Epoch 5, Training Loss 174.52645874023438\n",
      "Fold 2, Epoch 6, Training Loss 122.39900970458984\n",
      "Fold 2, Epoch 7, Training Loss 184.0079803466797\n",
      "Fold 2, Epoch 8, Training Loss 97.3982925415039\n",
      "Fold 2, Epoch 9, Training Loss 240.61038208007812\n",
      "Fold 2, Epoch 10, Training Loss 174.848876953125\n",
      "Fold 2, Epoch 11, Training Loss 102.2005844116211\n",
      "Fold 2, Epoch 12, Training Loss 117.59837341308594\n",
      "Fold 2, Epoch 13, Training Loss 124.54191589355469\n",
      "Fold 2, Epoch 14, Training Loss 117.81761169433594\n",
      "Fold 2, Epoch 15, Training Loss 155.0457763671875\n",
      "Fold 2, Epoch 16, Training Loss 105.09291076660156\n",
      "Fold 2, Epoch 17, Training Loss 161.8606719970703\n",
      "Fold 2, Epoch 18, Training Loss 218.1715545654297\n",
      "Fold 2, Epoch 19, Training Loss 118.93521881103516\n",
      "Fold 2, Epoch 20, Training Loss 211.12155151367188, Validation Loss 38.91124725341797\n",
      "Fold 2, Epoch 21, Training Loss 119.63645935058594\n",
      "Fold 2, Epoch 22, Training Loss 178.9793243408203\n",
      "Fold 2, Epoch 23, Training Loss 139.57554626464844\n",
      "Fold 2, Epoch 24, Training Loss 175.4638214111328\n",
      "Fold 2, Epoch 25, Training Loss 191.4521484375\n",
      "Fold 2, Epoch 26, Training Loss 154.69644165039062\n",
      "Fold 2, Epoch 27, Training Loss 160.27040100097656\n",
      "Fold 2, Epoch 28, Training Loss 171.53085327148438\n",
      "Fold 2, Epoch 29, Training Loss 113.89420318603516\n",
      "Fold 2, Epoch 30, Training Loss 126.28765106201172\n",
      "Fold 2, Epoch 31, Training Loss 106.23615264892578\n",
      "Fold 2, Epoch 32, Training Loss 107.84898376464844\n",
      "Fold 2, Epoch 33, Training Loss 90.00025939941406\n",
      "Fold 2, Epoch 34, Training Loss 108.17892456054688\n",
      "Fold 2, Epoch 35, Training Loss 122.04857635498047\n",
      "Fold 2, Epoch 36, Training Loss 82.73597717285156\n",
      "Fold 2, Epoch 37, Training Loss 119.78068542480469\n",
      "Fold 2, Epoch 38, Training Loss 145.3519744873047\n",
      "Fold 2, Epoch 39, Training Loss 77.96290588378906\n",
      "Fold 2, Epoch 40, Training Loss 149.57257080078125, Validation Loss 42.51795959472656\n",
      "Fold 2, Epoch 41, Training Loss 145.5626678466797\n",
      "Fold 2, Epoch 42, Training Loss 132.5957489013672\n",
      "Fold 2, Epoch 43, Training Loss 252.4170379638672\n",
      "Fold 2, Epoch 44, Training Loss 194.14874267578125\n",
      "Fold 2, Epoch 45, Training Loss 71.35673522949219\n",
      "Fold 2, Epoch 46, Training Loss 151.53225708007812\n",
      "Fold 2, Epoch 47, Training Loss 144.08929443359375\n",
      "Fold 2, Epoch 48, Training Loss 103.65360260009766\n",
      "Fold 2, Epoch 49, Training Loss 150.07846069335938\n",
      "Fold 2, Epoch 50, Training Loss 129.2870330810547\n",
      "Fold 2, Epoch 51, Training Loss 97.17286682128906\n",
      "Fold 2, Epoch 52, Training Loss 108.1727523803711\n",
      "Fold 2, Epoch 53, Training Loss 120.1837158203125\n",
      "Fold 2, Epoch 54, Training Loss 122.26766204833984\n",
      "Fold 2, Epoch 55, Training Loss 115.51294708251953\n",
      "Fold 2, Epoch 56, Training Loss 154.59506225585938\n",
      "Fold 2, Epoch 57, Training Loss 79.45640563964844\n",
      "Fold 2, Epoch 58, Training Loss 136.40036010742188\n",
      "Fold 2, Epoch 59, Training Loss 105.82929992675781\n",
      "Fold 2, Epoch 60, Training Loss 118.14818572998047, Validation Loss 50.80649948120117\n",
      "Fold 2, Epoch 61, Training Loss 127.17495727539062\n",
      "Fold 2, Epoch 62, Training Loss 116.4539794921875\n",
      "Fold 2, Epoch 63, Training Loss 91.85418701171875\n",
      "Fold 2, Epoch 64, Training Loss 141.14102172851562\n",
      "Fold 2, Epoch 65, Training Loss 112.45789337158203\n",
      "Fold 2, Epoch 66, Training Loss 169.06993103027344\n",
      "Fold 2, Epoch 67, Training Loss 143.0998077392578\n",
      "Fold 2, Epoch 68, Training Loss 66.74028015136719\n",
      "Fold 2, Epoch 69, Training Loss 106.23494720458984\n",
      "Fold 2, Epoch 70, Training Loss 120.39524841308594\n",
      "Fold 2, Epoch 71, Training Loss 49.70438766479492\n",
      "Fold 2, Epoch 72, Training Loss 84.4253158569336\n",
      "Fold 2, Epoch 73, Training Loss 86.72147369384766\n",
      "Fold 2, Epoch 74, Training Loss 69.2465591430664\n",
      "Fold 2, Epoch 75, Training Loss 79.99636840820312\n",
      "Fold 2, Epoch 76, Training Loss 79.54234313964844\n",
      "Fold 2, Epoch 77, Training Loss 104.20360565185547\n",
      "Fold 2, Epoch 78, Training Loss 91.33516693115234\n",
      "Fold 2, Epoch 79, Training Loss 127.1771469116211\n",
      "Fold 2, Epoch 80, Training Loss 87.79946899414062, Validation Loss 47.28411102294922\n",
      "Fold 2, Epoch 81, Training Loss 107.16413879394531\n",
      "Fold 2, Epoch 82, Training Loss 105.3602294921875\n",
      "Fold 2, Epoch 83, Training Loss 82.45247650146484\n",
      "Fold 2, Epoch 84, Training Loss 86.40570068359375\n",
      "Fold 2, Epoch 85, Training Loss 92.5641860961914\n",
      "Fold 2, Epoch 86, Training Loss 92.17335510253906\n",
      "Fold 2, Epoch 87, Training Loss 79.95490264892578\n",
      "Fold 2, Epoch 88, Training Loss 79.47543334960938\n",
      "Fold 2, Epoch 89, Training Loss 58.91951370239258\n",
      "Fold 2, Epoch 90, Training Loss 77.31080627441406\n",
      "Fold 2, Epoch 91, Training Loss 111.32088470458984\n",
      "Fold 2, Epoch 92, Training Loss 62.93911361694336\n",
      "Fold 2, Epoch 93, Training Loss 164.60003662109375\n",
      "Fold 2, Epoch 94, Training Loss 127.00947570800781\n",
      "Fold 2, Epoch 95, Training Loss 80.52617645263672\n",
      "Fold 2, Epoch 96, Training Loss 84.70989990234375\n",
      "Fold 2, Epoch 97, Training Loss 124.37275695800781\n",
      "Fold 2, Epoch 98, Training Loss 54.45273208618164\n",
      "Fold 2, Epoch 99, Training Loss 68.39051818847656\n",
      "Fold 2, Epoch 100, Training Loss 122.30302429199219, Validation Loss 69.26739501953125\n",
      "Fold 3, Epoch 0, Training Loss 69.96678924560547, Validation Loss 38.44869613647461\n",
      "Fold 3, Epoch 1, Training Loss 97.52426147460938\n",
      "Fold 3, Epoch 2, Training Loss 76.85910034179688\n",
      "Fold 3, Epoch 3, Training Loss 73.64453887939453\n",
      "Fold 3, Epoch 4, Training Loss 90.25545501708984\n",
      "Fold 3, Epoch 5, Training Loss 85.85326385498047\n",
      "Fold 3, Epoch 6, Training Loss 89.15541076660156\n",
      "Fold 3, Epoch 7, Training Loss 92.25032806396484\n",
      "Fold 3, Epoch 8, Training Loss 116.51095581054688\n",
      "Fold 3, Epoch 9, Training Loss 67.5967025756836\n",
      "Fold 3, Epoch 10, Training Loss 77.75312805175781\n",
      "Fold 3, Epoch 11, Training Loss 91.12285614013672\n",
      "Fold 3, Epoch 12, Training Loss 93.54769134521484\n",
      "Fold 3, Epoch 13, Training Loss 69.87432098388672\n",
      "Fold 3, Epoch 14, Training Loss 137.3186798095703\n",
      "Fold 3, Epoch 15, Training Loss 67.5185775756836\n",
      "Fold 3, Epoch 16, Training Loss 76.28285217285156\n",
      "Fold 3, Epoch 17, Training Loss 62.40440368652344\n",
      "Fold 3, Epoch 18, Training Loss 99.95296478271484\n",
      "Fold 3, Epoch 19, Training Loss 54.60153579711914\n",
      "Fold 3, Epoch 20, Training Loss 76.50804138183594, Validation Loss 56.42769241333008\n",
      "Fold 3, Epoch 21, Training Loss 108.4717788696289\n",
      "Fold 3, Epoch 22, Training Loss 56.850555419921875\n",
      "Fold 3, Epoch 23, Training Loss 86.82444763183594\n",
      "Fold 3, Epoch 24, Training Loss 76.41145324707031\n",
      "Fold 3, Epoch 25, Training Loss 89.57353210449219\n",
      "Fold 3, Epoch 26, Training Loss 81.5448989868164\n",
      "Fold 3, Epoch 27, Training Loss 91.39339447021484\n",
      "Fold 3, Epoch 28, Training Loss 173.3503875732422\n",
      "Fold 3, Epoch 29, Training Loss 87.50634765625\n",
      "Fold 3, Epoch 30, Training Loss 111.19843292236328\n",
      "Fold 3, Epoch 31, Training Loss 73.80452728271484\n",
      "Fold 3, Epoch 32, Training Loss 131.97840881347656\n",
      "Fold 3, Epoch 33, Training Loss 66.8364028930664\n",
      "Fold 3, Epoch 34, Training Loss 71.06037139892578\n",
      "Fold 3, Epoch 35, Training Loss 151.6758575439453\n",
      "Fold 3, Epoch 36, Training Loss 124.47200775146484\n",
      "Fold 3, Epoch 37, Training Loss 53.318115234375\n",
      "Fold 3, Epoch 38, Training Loss 49.64328384399414\n",
      "Fold 3, Epoch 39, Training Loss 114.42032623291016\n",
      "Fold 3, Epoch 40, Training Loss 73.44833374023438, Validation Loss 29.526437759399414\n",
      "Fold 3, Epoch 41, Training Loss 71.09373474121094\n",
      "Fold 3, Epoch 42, Training Loss 74.61016845703125\n",
      "Fold 3, Epoch 43, Training Loss 93.09591674804688\n",
      "Fold 3, Epoch 44, Training Loss 42.15699768066406\n",
      "Fold 3, Epoch 45, Training Loss 50.843475341796875\n",
      "Fold 3, Epoch 46, Training Loss 70.94821166992188\n",
      "Fold 3, Epoch 47, Training Loss 70.92740631103516\n",
      "Fold 3, Epoch 48, Training Loss 83.07420349121094\n",
      "Fold 3, Epoch 49, Training Loss 88.32879638671875\n",
      "Fold 3, Epoch 50, Training Loss 97.28681945800781\n",
      "Fold 3, Epoch 51, Training Loss 96.27764129638672\n",
      "Fold 3, Epoch 52, Training Loss 85.8822021484375\n",
      "Fold 3, Epoch 53, Training Loss 49.09894943237305\n",
      "Fold 3, Epoch 54, Training Loss 55.35992431640625\n",
      "Fold 3, Epoch 55, Training Loss 86.06613159179688\n",
      "Fold 3, Epoch 56, Training Loss 51.58401107788086\n",
      "Fold 3, Epoch 57, Training Loss 66.20995330810547\n",
      "Fold 3, Epoch 58, Training Loss 61.518699645996094\n",
      "Fold 3, Epoch 59, Training Loss 100.92195129394531\n",
      "Fold 3, Epoch 60, Training Loss 54.23408126831055, Validation Loss 48.6087532043457\n",
      "Fold 3, Epoch 61, Training Loss 68.24714660644531\n",
      "Fold 3, Epoch 62, Training Loss 60.23189163208008\n",
      "Fold 3, Epoch 63, Training Loss 83.31907653808594\n",
      "Fold 3, Epoch 64, Training Loss 79.50059509277344\n",
      "Fold 3, Epoch 65, Training Loss 80.88341522216797\n",
      "Fold 3, Epoch 66, Training Loss 108.98159790039062\n",
      "Fold 3, Epoch 67, Training Loss 106.91404724121094\n",
      "Fold 3, Epoch 68, Training Loss 101.76492309570312\n",
      "Fold 3, Epoch 69, Training Loss 40.119544982910156\n",
      "Fold 3, Epoch 70, Training Loss 62.19977569580078\n",
      "Fold 3, Epoch 71, Training Loss 59.023494720458984\n",
      "Fold 3, Epoch 72, Training Loss 50.39733123779297\n",
      "Fold 3, Epoch 73, Training Loss 66.5176773071289\n",
      "Fold 3, Epoch 74, Training Loss 98.80384826660156\n",
      "Fold 3, Epoch 75, Training Loss 61.09676742553711\n",
      "Fold 3, Epoch 76, Training Loss 75.87825775146484\n",
      "Fold 3, Epoch 77, Training Loss 55.23773193359375\n",
      "Fold 3, Epoch 78, Training Loss 99.68855285644531\n",
      "Fold 3, Epoch 79, Training Loss 53.42561721801758\n",
      "Fold 3, Epoch 80, Training Loss 41.82440948486328, Validation Loss 37.510345458984375\n",
      "Fold 3, Epoch 81, Training Loss 106.71730041503906\n",
      "Fold 3, Epoch 82, Training Loss 60.531192779541016\n",
      "Fold 3, Epoch 83, Training Loss 69.26197052001953\n",
      "Fold 3, Epoch 84, Training Loss 52.988136291503906\n",
      "Fold 3, Epoch 85, Training Loss 46.0731315612793\n",
      "Fold 3, Epoch 86, Training Loss 53.48969650268555\n",
      "Fold 3, Epoch 87, Training Loss 71.77066802978516\n",
      "Fold 3, Epoch 88, Training Loss 40.9941291809082\n",
      "Fold 3, Epoch 89, Training Loss 58.589210510253906\n",
      "Fold 3, Epoch 90, Training Loss 81.10699462890625\n",
      "Fold 3, Epoch 91, Training Loss 47.54486846923828\n",
      "Fold 3, Epoch 92, Training Loss 62.36756896972656\n",
      "Fold 3, Epoch 93, Training Loss 60.73550033569336\n",
      "Fold 3, Epoch 94, Training Loss 54.00365447998047\n",
      "Fold 3, Epoch 95, Training Loss 63.31576919555664\n",
      "Fold 3, Epoch 96, Training Loss 54.66077423095703\n",
      "Fold 3, Epoch 97, Training Loss 65.99220275878906\n",
      "Fold 3, Epoch 98, Training Loss 100.81436920166016\n",
      "Fold 3, Epoch 99, Training Loss 59.22494125366211\n",
      "Fold 3, Epoch 100, Training Loss 74.27403259277344, Validation Loss 43.07078552246094\n",
      "Fold 4, Epoch 0, Training Loss 92.80622863769531, Validation Loss 32.674564361572266\n",
      "Fold 4, Epoch 1, Training Loss 54.04178237915039\n",
      "Fold 4, Epoch 2, Training Loss 83.61874389648438\n",
      "Fold 4, Epoch 3, Training Loss 39.67268371582031\n",
      "Fold 4, Epoch 4, Training Loss 48.498558044433594\n",
      "Fold 4, Epoch 5, Training Loss 84.02630615234375\n",
      "Fold 4, Epoch 6, Training Loss 44.6993408203125\n",
      "Fold 4, Epoch 7, Training Loss 75.22332763671875\n",
      "Fold 4, Epoch 8, Training Loss 57.016761779785156\n",
      "Fold 4, Epoch 9, Training Loss 69.78351593017578\n",
      "Fold 4, Epoch 10, Training Loss 92.05506896972656\n",
      "Fold 4, Epoch 11, Training Loss 90.16386413574219\n",
      "Fold 4, Epoch 12, Training Loss 47.86371994018555\n",
      "Fold 4, Epoch 13, Training Loss 89.32487487792969\n",
      "Fold 4, Epoch 14, Training Loss 66.35589599609375\n",
      "Fold 4, Epoch 15, Training Loss 82.88811492919922\n",
      "Fold 4, Epoch 16, Training Loss 75.22525024414062\n",
      "Fold 4, Epoch 17, Training Loss 64.07546997070312\n",
      "Fold 4, Epoch 18, Training Loss 45.18150329589844\n",
      "Fold 4, Epoch 19, Training Loss 60.707950592041016\n",
      "Fold 4, Epoch 20, Training Loss 78.307861328125, Validation Loss 39.023555755615234\n",
      "Fold 4, Epoch 21, Training Loss 56.789669036865234\n",
      "Fold 4, Epoch 22, Training Loss 52.878326416015625\n",
      "Fold 4, Epoch 23, Training Loss 73.02204895019531\n",
      "Fold 4, Epoch 24, Training Loss 48.6775016784668\n",
      "Fold 4, Epoch 25, Training Loss 45.94234085083008\n",
      "Fold 4, Epoch 26, Training Loss 62.52809143066406\n",
      "Fold 4, Epoch 27, Training Loss 56.792362213134766\n",
      "Fold 4, Epoch 28, Training Loss 60.47258377075195\n",
      "Fold 4, Epoch 29, Training Loss 39.91948699951172\n",
      "Fold 4, Epoch 30, Training Loss 77.38726043701172\n",
      "Fold 4, Epoch 31, Training Loss 61.68794631958008\n",
      "Fold 4, Epoch 32, Training Loss 60.7923583984375\n",
      "Fold 4, Epoch 33, Training Loss 56.349178314208984\n",
      "Fold 4, Epoch 34, Training Loss 68.39470672607422\n",
      "Fold 4, Epoch 35, Training Loss 64.75559997558594\n",
      "Fold 4, Epoch 36, Training Loss 51.144630432128906\n",
      "Fold 4, Epoch 37, Training Loss 63.928672790527344\n",
      "Fold 4, Epoch 38, Training Loss 44.16702651977539\n",
      "Fold 4, Epoch 39, Training Loss 84.85688018798828\n",
      "Fold 4, Epoch 40, Training Loss 50.08004379272461, Validation Loss 24.38070297241211\n",
      "Fold 4, Epoch 41, Training Loss 46.40998840332031\n",
      "Fold 4, Epoch 42, Training Loss 61.49098205566406\n",
      "Fold 4, Epoch 43, Training Loss 82.64305114746094\n",
      "Fold 4, Epoch 44, Training Loss 56.0577278137207\n",
      "Fold 4, Epoch 45, Training Loss 36.17725372314453\n",
      "Fold 4, Epoch 46, Training Loss 46.491127014160156\n",
      "Fold 4, Epoch 47, Training Loss 39.00864028930664\n",
      "Fold 4, Epoch 48, Training Loss 39.75905990600586\n",
      "Fold 4, Epoch 49, Training Loss 42.5277099609375\n",
      "Fold 4, Epoch 50, Training Loss 68.55817413330078\n",
      "Fold 4, Epoch 51, Training Loss 42.572322845458984\n",
      "Fold 4, Epoch 52, Training Loss 55.15760803222656\n",
      "Fold 4, Epoch 53, Training Loss 68.806640625\n",
      "Fold 4, Epoch 54, Training Loss 67.38520812988281\n",
      "Fold 4, Epoch 55, Training Loss 73.86222076416016\n",
      "Fold 4, Epoch 56, Training Loss 59.02085876464844\n",
      "Fold 4, Epoch 57, Training Loss 42.3559684753418\n",
      "Fold 4, Epoch 58, Training Loss 87.18560028076172\n",
      "Fold 4, Epoch 59, Training Loss 56.706790924072266\n",
      "Fold 4, Epoch 60, Training Loss 68.3194580078125, Validation Loss 23.900527954101562\n",
      "Fold 4, Epoch 61, Training Loss 55.510643005371094\n",
      "Fold 4, Epoch 62, Training Loss 50.18634033203125\n",
      "Fold 4, Epoch 63, Training Loss 52.19392013549805\n",
      "Fold 4, Epoch 64, Training Loss 77.70446014404297\n",
      "Fold 4, Epoch 65, Training Loss 41.9512939453125\n",
      "Fold 4, Epoch 66, Training Loss 30.67942237854004\n",
      "Fold 4, Epoch 67, Training Loss 86.7315902709961\n",
      "Fold 4, Epoch 68, Training Loss 57.23938751220703\n",
      "Fold 4, Epoch 69, Training Loss 60.5843620300293\n",
      "Fold 4, Epoch 70, Training Loss 74.44024658203125\n",
      "Fold 4, Epoch 71, Training Loss 42.1265869140625\n",
      "Fold 4, Epoch 72, Training Loss 62.6070442199707\n",
      "Fold 4, Epoch 73, Training Loss 80.93222045898438\n",
      "Fold 4, Epoch 74, Training Loss 38.212158203125\n",
      "Fold 4, Epoch 75, Training Loss 35.153160095214844\n",
      "Fold 4, Epoch 76, Training Loss 42.76325225830078\n",
      "Fold 4, Epoch 77, Training Loss 67.95561218261719\n",
      "Fold 4, Epoch 78, Training Loss 43.39639663696289\n",
      "Fold 4, Epoch 79, Training Loss 52.621803283691406\n",
      "Fold 4, Epoch 80, Training Loss 35.14164733886719, Validation Loss 33.0376091003418\n",
      "Fold 4, Epoch 81, Training Loss 59.37360382080078\n",
      "Fold 4, Epoch 82, Training Loss 52.688385009765625\n",
      "Fold 4, Epoch 83, Training Loss 51.61433792114258\n",
      "Fold 4, Epoch 84, Training Loss 79.34469604492188\n",
      "Fold 4, Epoch 85, Training Loss 49.76294708251953\n",
      "Fold 4, Epoch 86, Training Loss 53.778053283691406\n",
      "Fold 4, Epoch 87, Training Loss 57.058135986328125\n",
      "Fold 4, Epoch 88, Training Loss 77.8785400390625\n",
      "Fold 4, Epoch 89, Training Loss 49.42156219482422\n",
      "Fold 4, Epoch 90, Training Loss 43.793418884277344\n",
      "Fold 4, Epoch 91, Training Loss 51.17182922363281\n",
      "Fold 4, Epoch 92, Training Loss 46.1778564453125\n",
      "Fold 4, Epoch 93, Training Loss 64.90203094482422\n",
      "Fold 4, Epoch 94, Training Loss 53.50745391845703\n",
      "Fold 4, Epoch 95, Training Loss 65.81038665771484\n",
      "Fold 4, Epoch 96, Training Loss 64.03545379638672\n",
      "Fold 4, Epoch 97, Training Loss 76.86076354980469\n",
      "Fold 4, Epoch 98, Training Loss 54.34255599975586\n",
      "Fold 4, Epoch 99, Training Loss 87.55545043945312\n",
      "Fold 4, Epoch 100, Training Loss 76.67818450927734, Validation Loss 56.467010498046875\n",
      "Fold 5, Epoch 0, Training Loss 48.773704528808594, Validation Loss 12.658551216125488\n",
      "Fold 5, Epoch 1, Training Loss 89.88697814941406\n",
      "Fold 5, Epoch 2, Training Loss 33.54640579223633\n",
      "Fold 5, Epoch 3, Training Loss 61.01039505004883\n",
      "Fold 5, Epoch 4, Training Loss 32.33822250366211\n",
      "Fold 5, Epoch 5, Training Loss 40.71354675292969\n",
      "Fold 5, Epoch 6, Training Loss 51.69403839111328\n",
      "Fold 5, Epoch 7, Training Loss 51.14576721191406\n",
      "Fold 5, Epoch 8, Training Loss 37.480552673339844\n",
      "Fold 5, Epoch 9, Training Loss 52.33617401123047\n",
      "Fold 5, Epoch 10, Training Loss 87.40760040283203\n",
      "Fold 5, Epoch 11, Training Loss 40.41905212402344\n",
      "Fold 5, Epoch 12, Training Loss 60.59996795654297\n",
      "Fold 5, Epoch 13, Training Loss 71.95767211914062\n",
      "Fold 5, Epoch 14, Training Loss 48.05957794189453\n",
      "Fold 5, Epoch 15, Training Loss 45.14028549194336\n",
      "Fold 5, Epoch 16, Training Loss 43.4561882019043\n",
      "Fold 5, Epoch 17, Training Loss 23.047813415527344\n",
      "Fold 5, Epoch 18, Training Loss 61.36786651611328\n",
      "Fold 5, Epoch 19, Training Loss 84.7488021850586\n",
      "Fold 5, Epoch 20, Training Loss 35.50099563598633, Validation Loss 21.77027130126953\n",
      "Fold 5, Epoch 21, Training Loss 72.26949310302734\n",
      "Fold 5, Epoch 22, Training Loss 41.71826934814453\n",
      "Fold 5, Epoch 23, Training Loss 54.48927688598633\n",
      "Fold 5, Epoch 24, Training Loss 45.579105377197266\n",
      "Fold 5, Epoch 25, Training Loss 80.49190521240234\n",
      "Fold 5, Epoch 26, Training Loss 36.35319519042969\n",
      "Fold 5, Epoch 27, Training Loss 48.679500579833984\n",
      "Fold 5, Epoch 28, Training Loss 72.13787078857422\n",
      "Fold 5, Epoch 29, Training Loss 30.83921241760254\n",
      "Fold 5, Epoch 30, Training Loss 98.13697052001953\n",
      "Fold 5, Epoch 31, Training Loss 57.550559997558594\n",
      "Fold 5, Epoch 32, Training Loss 35.71713638305664\n",
      "Fold 5, Epoch 33, Training Loss 50.0299072265625\n",
      "Fold 5, Epoch 34, Training Loss 59.57045364379883\n",
      "Fold 5, Epoch 35, Training Loss 59.635841369628906\n",
      "Fold 5, Epoch 36, Training Loss 48.07688522338867\n",
      "Fold 5, Epoch 37, Training Loss 42.01730728149414\n",
      "Fold 5, Epoch 38, Training Loss 36.672061920166016\n",
      "Fold 5, Epoch 39, Training Loss 23.14225959777832\n",
      "Fold 5, Epoch 40, Training Loss 49.78638458251953, Validation Loss 22.019428253173828\n",
      "Fold 5, Epoch 41, Training Loss 44.94258117675781\n",
      "Fold 5, Epoch 42, Training Loss 22.192184448242188\n",
      "Fold 5, Epoch 43, Training Loss 49.9451904296875\n",
      "Fold 5, Epoch 44, Training Loss 55.213600158691406\n",
      "Fold 5, Epoch 45, Training Loss 40.180580139160156\n",
      "Fold 5, Epoch 46, Training Loss 47.2452392578125\n",
      "Fold 5, Epoch 47, Training Loss 70.24617004394531\n",
      "Fold 5, Epoch 48, Training Loss 62.12611389160156\n",
      "Fold 5, Epoch 49, Training Loss 53.85908508300781\n",
      "Fold 5, Epoch 50, Training Loss 47.259056091308594\n",
      "Fold 5, Epoch 51, Training Loss 64.24622344970703\n",
      "Fold 5, Epoch 52, Training Loss 30.771286010742188\n",
      "Fold 5, Epoch 53, Training Loss 43.904727935791016\n",
      "Fold 5, Epoch 54, Training Loss 62.138065338134766\n",
      "Fold 5, Epoch 55, Training Loss 94.37814331054688\n",
      "Fold 5, Epoch 56, Training Loss 41.580020904541016\n",
      "Fold 5, Epoch 57, Training Loss 37.714508056640625\n",
      "Fold 5, Epoch 58, Training Loss 32.88576126098633\n",
      "Fold 5, Epoch 59, Training Loss 49.28207015991211\n",
      "Fold 5, Epoch 60, Training Loss 57.58998489379883, Validation Loss 23.312435150146484\n",
      "Fold 5, Epoch 61, Training Loss 64.5579605102539\n",
      "Fold 5, Epoch 62, Training Loss 68.1275634765625\n",
      "Fold 5, Epoch 63, Training Loss 56.13714599609375\n",
      "Fold 5, Epoch 64, Training Loss 80.10514831542969\n",
      "Fold 5, Epoch 65, Training Loss 54.88801574707031\n",
      "Fold 5, Epoch 66, Training Loss 37.24803161621094\n",
      "Fold 5, Epoch 67, Training Loss 39.35683822631836\n",
      "Fold 5, Epoch 68, Training Loss 49.10600662231445\n",
      "Fold 5, Epoch 69, Training Loss 46.36315155029297\n",
      "Fold 5, Epoch 70, Training Loss 40.21159744262695\n",
      "Fold 5, Epoch 71, Training Loss 56.275482177734375\n",
      "Fold 5, Epoch 72, Training Loss 42.49405288696289\n",
      "Fold 5, Epoch 73, Training Loss 43.09761047363281\n",
      "Fold 5, Epoch 74, Training Loss 30.171995162963867\n",
      "Fold 5, Epoch 75, Training Loss 27.956937789916992\n",
      "Fold 5, Epoch 76, Training Loss 50.303035736083984\n",
      "Fold 5, Epoch 77, Training Loss 31.722122192382812\n",
      "Fold 5, Epoch 78, Training Loss 29.550548553466797\n",
      "Fold 5, Epoch 79, Training Loss 84.55819702148438\n",
      "Fold 5, Epoch 80, Training Loss 46.08976745605469, Validation Loss 52.95512390136719\n",
      "Fold 5, Epoch 81, Training Loss 63.367767333984375\n",
      "Fold 5, Epoch 82, Training Loss 83.0039291381836\n",
      "Fold 5, Epoch 83, Training Loss 41.35985565185547\n",
      "Fold 5, Epoch 84, Training Loss 81.40283966064453\n",
      "Fold 5, Epoch 85, Training Loss 34.43586349487305\n",
      "Fold 5, Epoch 86, Training Loss 37.025978088378906\n",
      "Fold 5, Epoch 87, Training Loss 58.68532943725586\n",
      "Fold 5, Epoch 88, Training Loss 60.2606315612793\n",
      "Fold 5, Epoch 89, Training Loss 32.26982879638672\n",
      "Fold 5, Epoch 90, Training Loss 54.316226959228516\n",
      "Fold 5, Epoch 91, Training Loss 63.823970794677734\n",
      "Fold 5, Epoch 92, Training Loss 38.947906494140625\n",
      "Fold 5, Epoch 93, Training Loss 37.686763763427734\n",
      "Fold 5, Epoch 94, Training Loss 58.971717834472656\n",
      "Fold 5, Epoch 95, Training Loss 47.981712341308594\n",
      "Fold 5, Epoch 96, Training Loss 34.232017517089844\n",
      "Fold 5, Epoch 97, Training Loss 76.26506042480469\n",
      "Fold 5, Epoch 98, Training Loss 28.470006942749023\n",
      "Fold 5, Epoch 99, Training Loss 48.51716613769531\n",
      "Fold 5, Epoch 100, Training Loss 68.08627319335938, Validation Loss 45.365966796875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# Use small data for hyperparameter tuning\n",
    "# dataset_size = len(test_data)\n",
    "dataset_size = len(train_data)\n",
    "\n",
    "indices = list(range(dataset_size))\n",
    "history = []\n",
    "# Training loop function with cross-validation\n",
    "def train_model_with_cv(model, dataset, criterion, optimizer, epochs=100):\n",
    "    for fold, (train_indices, val_indices) in enumerate(kfold.split(indices)):\n",
    "        train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "        val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "        fold_history = []\n",
    "        for epoch in range(epochs+1):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % 20 == 0:\n",
    "                model.eval()\n",
    "                #with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    val_loss = criterion(outputs, labels)\n",
    "                print('Fold {}, Epoch {}, Training Loss {}, Validation Loss {}'.format(fold+1, epoch, loss.item(), val_loss.item()))\n",
    "            else:\n",
    "                print('Fold {}, Epoch {}, Training Loss {}'.format(fold+1, epoch, loss.item()))\n",
    "            fold_history.append(loss.item())\n",
    "        history.append(fold_history)\n",
    "\n",
    "# Testing\n",
    "#train_model_with_cv(model, test_data, criterion, optimizer)\n",
    "train_model_with_cv(model, train_data, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef8464d7-7234-429c-9161-0bc74bd40be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e57765ec-3919-4749-ae65-afa99f918bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.54234313964844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a54bff50>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = []\n",
    "for i in history:\n",
    "    graph += i\n",
    "print(statistics.median(graph))\n",
    "plt.plot(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679397e9-64be-45bb-8c17-0c5c70d1309e",
   "metadata": {},
   "source": [
    "## 3. Prediction and Other Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb04bc5-26f5-4176-9840-c2fe61d3be41",
   "metadata": {},
   "source": [
    "### 3.1 Prediction of the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf263517-5569-4671-a766-63a8824c25c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy with ±10 Tolerance: 88.09%\n",
      "Predictions: [96.78453063964844, 89.34066009521484, 109.46536254882812, 100.83441162109375, 74.86923217773438]\n",
      "Actual Values: [95.7033462524414, 92.9509506225586, 99.80580139160156, 80.27259826660156, 78.0486068725586]\n"
     ]
    }
   ],
   "source": [
    "def predict_and_evaluate_accuracy(model, test_loader, tolerance=10.0):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predictions.extend(outputs.tolist())\n",
    "            actual_values.extend(labels.squeeze().tolist())\n",
    "\n",
    "            # Calculate accuracy within the tolerance range\n",
    "            correct_predictions += torch.sum(torch.abs(outputs - labels.squeeze()) <= tolerance)\n",
    "            total_predictions += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions.float() / total_predictions * 100\n",
    "\n",
    "    return predictions, actual_values, accuracy.item()\n",
    "\n",
    "predictions, actual_values, test_accuracy = predict_and_evaluate_accuracy(model, test_loader)\n",
    "print(f'Test Accuracy with ±10 Tolerance: {test_accuracy:.2f}%')\n",
    "\n",
    "print(\"Predictions:\", predictions[0:5])\n",
    "print(\"Actual Values:\", actual_values[0:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309bd9e9-3ba2-4db9-8299-4130354a5f4f",
   "metadata": {},
   "source": [
    "### 3.2 Get Weight Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8e0f5-d35e-4823-985c-c363f801c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af5a7e-1161-47d7-a4d0-1397f971df5c",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47a2d7-bcc1-4dbe-9f4f-52be2d2f45a4",
   "metadata": {},
   "source": [
    "* Our model has a average loss of 70, pretty comparable to the published literature value (loss: 33~133)\n",
    "* Tolerance +- 10 is sufficient enough for the application of setting the dynamic window for a targeted peptide on the instrument (mass spectrometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52694f-65f9-475f-9ad5-b6f3a59c7a65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
